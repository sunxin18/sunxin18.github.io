<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[520随笔]]></title>
    <url>%2F2020%2F05%2F20%2F520%2F</url>
    <content type="text"><![CDATA[哎别看了。 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]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[seq2seq]]></title>
    <url>%2F2020%2F05%2F20%2Fseq2%2F</url>
    <content type="text"><![CDATA[本文将介绍seq2seq模型以及机器翻译的内容 seq2seq seq2seq(sequence to sequence)故名思意就是一种能够根据给定的序列，通过特定的方法生成另一个序列的方法，属于encoder-decoder结构的一种，这里看看常见的encoder-decoder结构，基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder。encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量C。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。 而decoder则负责根据语义向量生成指定的序列，这个过程也称为解码，如下图，最简单的方式是将encoder得到的语义变量作为初始状态输入到decoder的RNN中，得到输出序列。可以看到上一时刻的输出会作为当前时刻的输入，而且其中语义向量C只作为初始状态参与运算，后面的运算都与语义向量C无关。 seq2seq应用在机器翻译、对话机器人等领域，后续会补充一些项目实战，下面先介绍一下基础思路 seq2seq训练 训练数据为&lt;中文，英文&gt;，然后我们将每个minibatch中语句的长度补成相同（与最大长度相同），不同的minibatch不要求相同长度，这样方便进行向量化矩阵化(matrix)操作， infereence/Decoding 如果我们已经训练好了模型参数，怎么生成语句序列呢呢？ 通过模型softmax，我们可以得到每个单词的概率分布，然后每次选取概率最大的作为生成词(greedy)，但如果我们只考虑每个单词(unigram)，即使生成的语句单词相同，但是如果顺序不同就会不通顺，即没有好的语法结构，所以可以用bgram。 如何改进只能考虑局部最优解的局限性呢？下面有两个改进想法，exhaustic search,beam search exhaustic search 就是每次都考虑所有单词的情况，而不是只选择概率最高的单词，这样肯定可以找到全局最优解，但是这样就会复杂度很高，为O(∣v∣T)O(\left | v\right |^{T})O(∣v∣T) beam search exhaustic search复杂度太高，我们考虑使用beam search，这种方法是既考虑了贪心算法的改进也对复杂度降低，如果我们设三种方法的最优解是ToT_{o}To​,TeT_{e}Te​,TbT_{b}Tb​,那么To⊂Te⊂TbT_{o} \subset T_{e} \subset T_{b}To​⊂Te​⊂Tb​ beam search的想法是每次只考虑最好的k个单词，接下来是对第一次最好的k个单词继续考虑第二个生成单词也是取概率最高的前k个,但如果这样继续下去又是指数级增长了，所以是取对目前的序列（目前也就是长度为2）的概率和(其实是概率的乘积，但因为取了log)最高的k个序列，最后直到序列预测到end时停止，这样考虑也是有局限的，因为可能有的序列到很短就停止了，这样他的值就很大，最优解会优先选他，所以这块我们对每个序列除以长度。 每一步最多考虑k2k^{2}k2个可能性，这个算法的复杂度为O(k2T)O(k^{2}T)O(k2T)]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[attention(注意力机制解析)]]></title>
    <url>%2F2020%2F05%2F20%2Fattention%2F</url>
    <content type="text"><![CDATA[本文我们将讲解注意力机制在视觉和自然语言以及图挖掘领域的使用。 PyTorch：https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning mutimodal learning 即将多个模型连接起来，每个模型完成相应的任务，一起完成比如看图说话、提取摘要等任务。 attention(注意力机制) 将注意力放到重要的地方，比较符合人类的思考，可以应用在文本、图像、自注意力(self-attention) 看图说话 先把图片提取出一个向量作为桥梁，我们取模型中倒数第二个向量v，wih是一个转移矩阵,输入到RNN中来生成语句序列 但这模型有些问题， 识别不出来某些物体 我们把图片等价于一个向量，通过这个向量来生成，其实有时候我们只需要关注图片中的一直猫，不需要理解整张图片 图像识别的注意力机制 这里feature矩阵每个格子都是一个d维的向量，a1也是一个3×3的矩阵，每个格代表这个区域的权重，当我们生成第一个单词我们要将注意力放在概率最高的区域，然后将这个两个矩阵的信息进行汇集到向量ZZZ，较于之前的seq2seq模型，我们这里就多了一个ZZZ 我们将z1z1z1输入到rnn模型，这里的first word就是start ，通过h1输出第一个单词和第二个单词的注意力，然后再生成z2z2z2,这里的y2y2y2就是的d1d1d1 https://blog.csdn.net/shenxiaolu1984/article/details/51493673#fn:1 attention in seq2seq seq2seq的不足： 梯度问题 首先我们的语义编码c，我们的目的是希望这个向量能包括所有输入的单词信息，但是因为LSTM梯度消失的问题，c可能只能捕获离他较近的几个单词的信息，前面的单词可能就考虑不到了(怎么改进？给前面的单词分配一个权重——–attention) 应用角度 现在的模型，我们生成下一个单词是依赖于上一个单词(pre)以及语义编码c,即v←(pre,c)v \leftarrow (pre,c)v←(pre,c)，然而往往我们在翻译的时候只会关注句子的一部分，并且这种模式太过于依赖语义编码c，如果c的效果不理想，那么最后的decoder也会不理想，这也是bottleneck problem。还有一个原因，c向量的维度已经固定，对不同长度的输入句子，都用一个固定维度的向量表达难免不足 seq2seq的attention 下面这幅图讲解了大致流程，首先看encoder部分这里的隐含向量g是什么，举个例子，按照图的描述，www就是transformation,f是激活函数，g_{2}=f(W_{xh} \cdot embeding(weather)+W_{hh}\cdot g_{1})。我们要使用注意力，就要给g向量一个权重，最简单的就是。 我们要使用注意力，就要给g向量一个权重，最简单的就是。我们要使用注意力，就要给g向量一个权重，最简单的就是h和和和g的内积，当然也可以设置别的函数，比如的内积，当然也可以设置别的函数，比如的内积，当然也可以设置别的函数，比如g^{T} \cdot M \cdot h,这里的M就是要学习的一个参数矩阵。我们以内积的方法为例，通过计算每个,这里的M就是要学习的一个参数矩阵。 我们以内积的方法为例，通过计算每个,这里的M就是要学习的一个参数矩阵。我们以内积的方法为例，通过计算每个 g^{T} \cdot h $我们得到了每个单词的一个权重，然后通过归一化(可以是简单的通分，也可以是softmax)。 现在我们如何生成单词呢？首先先通过attention,来生成新的context vector c1c_{1}c1​,用c1c_{1}c1​来生成(之前我们用的是ccc),c1c_{1}c1​按图中的例子就是c1=0.6⋅g1+0.2⋅g2+0.1⋅g3+0.1⋅g4c_{1}=0.6\cdot g_{1}+0.2\cdot g_{2}+0.1\cdot g_{3}+0.1\cdot g_{4}c1​=0.6⋅g1​+0.2⋅g2​+0.1⋅g3​+0.1⋅g4​，然后我们将c1c_{1}c1​和h1h_{1}h1​拼接起来，y1=softmax(Why⋅(c1∣h1+b)y_{1}=softmax(W_{hy} \cdot (c_{1}|h_{1}+b)y1​=softmax(Why​⋅(c1​∣h1​+b)(y1y_{1}y1​就是v维度的,选取概率最大的单词作为预测词),之后类似，y1y_{1}y1​作为第二个预测单词的输入 attention优点： 解决了梯度问题 可解释性. 没有attention，我们无法可视化，无法看到模型的不足点去修改 attention in seq2seq的不足： 我们这种思路是给每个单词一个权重，我们更希望每个单词是独立的，然而RNN的特性，当前单词又会汇集之前的单词信息，这也是一个矛盾点， 想法一：**g2-g1?来去除冗余信息？**当然这是不对的，因为向量的表达相当于是空间中的一个位置，向量的差值没有什么含义，有方向的，方向是不能减的。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法优化]]></title>
    <url>%2F2020%2F05%2F15%2Foptimize%2F</url>
    <content type="text"><![CDATA[本系列将探讨算法的优化。 思维的改进 LCP08.剧情触发时间 最开始的想法同时考虑三个属性，每到新的一天就遍历requirements数组，看能否有剧情出发，果不其然超时了，最后两个样例没通过 之后进行改进。 首先如果只考虑一种属性，显然我们只需要计算每一天的属性情况，最后对于所有的requirements 都在属性列表中进行二分查找（现在只有一种属性了)，就能知道他是在哪一天完成的了。 但这道题实际上，每一种属性的满足是互相独立的。 简单来说，对于一个剧情要求 (C, R, H) 来说，假设 C 要求是在第 x 天满足的，R要求是在第 y 天满足的，H 要求是在第 z 天满足的。那么该剧情的满足时间为： t=max(x,y,z) 原始代码： c++12345678910111213141516171819202122232425262728293031class Solution &#123;public: vector&lt;int&gt; getTriggerTime(vector&lt;vector&lt;int&gt;&gt;&amp; increase, vector&lt;vector&lt;int&gt;&gt;&amp; requirements) &#123; int res[3]=&#123;0&#125;; int day=increase.size(); vector&lt;int&gt;end(requirements.size(),-1); for(int i=0;i&lt;day;i++)&#123; for(int j=0;j&lt;3;j++)&#123; res[j]+=increase[i][j]; &#125; for(int m=0;m&lt;requirements.size();m++)&#123; if(end[m]!=-1)continue; int count=0; int count1=0; for(int n=0;n&lt;3;n++)&#123; if(res[n]&gt;=requirements[m][n]) count++; if(requirements[m][n]==0) count1++; &#125; if(count1==3)&#123; end[m]=0; continue; &#125; if(count==3) end[m]=i+1; &#125; &#125; return end; &#125;&#125;; 改进代码： c++123456789101112131415161718192021222324252627282930313233class Solution &#123;public: vector&lt;int&gt; getTriggerTime(vector&lt;vector&lt;int&gt;&gt;&amp; increase, vector&lt;vector&lt;int&gt;&gt;&amp; requirements) &#123; //将三个属性分开，并初始化为0 vector&lt;int&gt; C(increase.size() + 1, 0);//初始化 vector&lt;int&gt; R(increase.size() + 1, 0); vector&lt;int&gt; H(increase.size() + 1, 0); for (int i = 0; i &lt; increase.size(); ++i)//算和 &#123; C[i + 1] = C[i] + increase[i][0]; R[i + 1] = R[i] + increase[i][1]; H[i + 1] = H[i] + increase[i][2]; &#125; vector&lt;int&gt; ret;//返回值 int maxlen = C.size();//若lower_bound中没找到则返回last，此时求的差是数组长度 for (int i = 0; i &lt; requirements.size(); i++) &#123; //lower_bound 返回大于等于查找元素的位置 int lbc = lower_bound(C.begin(), C.end(), requirements[i][0]) - C.begin(); int lbr = lower_bound(R.begin(), R.end(), requirements[i][1]) - R.begin(); int lbh = lower_bound(H.begin(), H.end(), requirements[i][2]) - H.begin(); if (lbc == maxlen || lbr == maxlen || lbh == maxlen)//没触发 &#123; ret.emplace_back(-1); &#125; else &#123; ret.emplace_back(max(max(lbc, lbr), lbh)); &#125; &#125; return ret; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode多函数编程]]></title>
    <url>%2F2020%2F05%2F15%2Ffunction%2F</url>
    <content type="text"><![CDATA[设计地铁系统 这道题主要思考点在于容器设计，in容器来记录编号id的人的（上车站点和上车时间）–pair，然后不需要额外用out来记录下车，直接在checkout函数里计算一下id的人上下车时间差做好记录，最后直接求mean就可以啦 c++1234567891011121314151617181920212223242526272829class UndergroundSystem &#123;public: using pii = pair&lt;string, int&gt;; unordered_map&lt;int, pii&gt; in; map&lt;pair&lt;string,string&gt;, int&gt; a, b; UndergroundSystem() &#123; in.clear(); a.clear(); b.clear(); &#125; void checkIn(int id, string stationName, int t) &#123; in[id] = &#123;stationName, t&#125;; &#125; void checkOut(int id, string stationName, int t) &#123; auto [ss, tt] = in[id]; int time = t-tt; a[&#123;ss,stationName&#125;] ++; b[&#123;ss,stationName&#125;] += time; &#125; double getAverageTime(string s, string t) &#123; return (double)b[&#123;s,t&#125;]/a[&#123;s,t&#125;]; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>多函数编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Search]]></title>
    <url>%2F2020%2F05%2F09%2FBinary-Search%2F</url>
    <content type="text"><![CDATA[二分搜索核心思想：把待搜索区间分为有目标元素的区间和不包含目标元素的区间，排除掉不包含目标元素的区间的区间，剩下就是有目标元素的区间。]]></content>
  </entry>
  <entry>
    <title><![CDATA[华为云口罩配送大赛经验分享]]></title>
    <url>%2F2020%2F05%2F08%2Fhuawei%2F</url>
    <content type="text"><![CDATA[比赛链接：https://competition.huaweicloud.com/information/1000037176/introduction 赛题分析： 地图大小为12×12，需求点固定为5个，配送目标是将所有需求点的需求进行满足，配送过程中会随机生成捐赠小区 纯命令行交互，使用标准I/O作为命令（S/R/G）和移动方向(E/W/S/N)的传递途径 在1000张地图上测试 思路 我没有做太多的思考，思路也很容易理解，首先先到的就是greedy策略，每次都选择当前最好的选择，当然这只能考虑到局部最优，但最后结果还不错，能排进前二十。 每次配送的时候都优先选择离当前最近的需求点配送，取货的时候给每个捐赠小区一个ranking，就按照捐赠数量/距离来做，试了一下距离的平方结果不如绝对值。 配送有几种情况需要考虑： 车上口罩为0，那么肯定要去取货，那么就按ranking来选择 车上口罩为100，那么肯定要去送货，就选择最近的，因为最远的周边可能后续会生成捐赠小区 接下的两种情况最难考虑，就是送完一个小区或者刚取完一些车上还剩，那么是去接着送别的小区，还是取货呢？我的想法就是也写一个ranking比较，送货和取货的价值比较，但要考虑一些特殊情况，如果此时的货够最近的需求小区a那么就去送，如果此时的货加上离aranking最高的取货点足够那么就直接去取货点取货，当然这考虑非常不够，很需要改进,可以学习依一下别的选手的思路。 代码分析 下面对我的代码进行讲解，用python写的. load:当前装载量 target：目标地 R(字典)：对应坐标的货量，需求点就是负值，捐赠和仓库就是正值，坐标要用元组，不是列表 主函数 需求小区仓库初始化 python12345678910111213if __name__ == '__main__': new_string=input() s_list=new_string.split() S=(int(s_list[1]),int(s_list[2])) R=&#123;&#125; R[(int(s_list[1]),int(s_list[2]))]=100 for i in range(0,5): new_string=input() r_list=new_string.split() R[(int(r_list[1]),int(r_list[2]))]=int(r_list[3]) p=list(S) load=0 #初始化装载量为0 target=S 选择（每次到达小区或者捐赠点就行下一步的选择) 这里我加入了一个配送时如果有顺路的捐赠小区，那就去取一下 python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950while True: P=tuple(p) if P == S: load = 100 target = choose_na_target(p) elif P in R.keys(): sum=load+R[P] if R[P] &lt; 0: if sum&lt;0: load=0 R[P]=sum if R[closest(p)] &gt;= -R[P]: target = closest(p) else: target= choose_target(p) #车空就去送取货 else: del R[P] if len(R)==0: break if min(R.values())&gt;0: break load=sum target1 = choose_na_target(p) target2 = choose_target(p) target3 = choose_target(target1) if load &gt;= -R[target1]: target = target1 elif load + R[target3] &gt;= -R[target1]: target = target3 else: target = com_value(p ,target1,target2) else: if sum &gt; 100: load=100 R[P]=sum-100 target = choose_na_target(p) else: del R[P] load=sum target1 = choose_na_target(p) target2 = choose_target(p) if load &gt;= -R[target1]: target = target1 else: target = com_value(p ,target1,target2) for k in R.keys(): #顺路就去取 if R[k] &gt; 0: if(k[0]&gt;=target[0] and k[0]&lt;=p[0]) or (k[0]&lt;=target[0] and k[0]&gt;=p[0]): if(k[1]&gt;=target[1] and k[1]&lt;=p[1]) or (k[1]&lt;=target[1] and k[1]&gt;=p[1]): target = k 读取命令行输入 python12345678910111213141516171819new_string=input() if new_string=='G': #读取到行动的命令，向target移动 p,next_step=step(p,target) print(next_step) else: r_list=new_string.split() if p[0] == int(r_list[1]) and p[1] == int(r_list[2]): #空投贴脸hhh if load + int(r_list[3]) &lt;= 100: load = load + int(r_list[3]) else: R[(int(r_list[1]),int(r_list[2]))]=int(r_list[3])-100+load load = 100 else: R[(int(r_list[1]),int(r_list[2]))]=int(r_list[3]) #更新一下选择 if R[target] &gt; 0: #注意R里没有S target = choose_target(p) else: cur_target = (int(r_list[1]),int(r_list[2])) target = com_value(p ,target,cur_target) 移动函数 python1234567891011121314def step(pos,to): if pos[0]&lt;to[0]: pos[0]=pos[0]+1 return pos,'S' elif pos[0]&gt;to[0]: pos[0]=pos[0]-1 return pos,'N' else: if pos[1]&lt;to[1]: pos[1]=pos[1]+1 return pos,'E' else: pos[1]=pos[1]-1 return pos,'W' 选择捐赠小区和需求小区 python12345678910111213141516171819202122232425262728293031323334def choose_na_target(pos): na_distance=0 global load for k in R.keys(): if R[k] &lt; 0: if k[0]==pos[0] and k[1]==pos[1]: continue cur_dis2 = 1/ (abs(pos[0]-k[0])+abs(pos[1]-k[1])+1) if cur_dis2 &gt; na_distance: na_distance = cur_dis2 tar = k return tar```pythondef choose_target(pos): po_distance=0 global load distance = (100-load)/ (abs(pos[0]-S[0])+abs(pos[1]-S[1])+1) for k in R.keys(): if R[k] &gt; 0: if k[0]==pos[0] and k[1]==pos[1]: continue if R[k] + load &gt; 100: cur_dis1 = (100-load)/ (abs(pos[0]-k[0])+abs(pos[1]-k[1])+1) else: cur_dis1 = R[k] / (abs(pos[0] - k[0]) + abs(pos[1] - k[1])+1) if cur_dis1 &gt; po_distance: po_distance = cur_dis1 tar =k if po_distance&gt; distance: return tar else: return S 捐赠小区和需求小区的比较ranking python12345678910111213141516def com_value(pos,target1,target2): global target if load &lt; -R[target1]: cur_value1 = load else: cur_value1 = -R[target1] value1 = cur_value1 / (abs(pos[0]-target1[0])+abs(pos[1]-target1[1])+1) if load +R[target2] &gt; 100: cur_value2 = 100 -load else: cur_value2 = R[target2] value2 = cur_value2 / (abs(pos[0]-target2[0])+abs(pos[1]-target2[1])+1) if value1 &gt;value2: return target1 else: return target2]]></content>
      <tags>
        <tag>比赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换新电脑hexo博客的迁移]]></title>
    <url>%2F2020%2F04%2F29%2Fblog-trsf%2F</url>
    <content type="text"><![CDATA[一、拷贝原始电脑blog文件夹到新电脑 二、安装git、node.js 三、安装 hexo 在 cmd 下输入下面指令安装 hexo Codeinstall hexo-cli -g```1234567## 四、在blog文件夹下执行git bash，输入以下命令```npm installnpm install hexo-deployer-git --save // 文章部署到 git 的模块（下面为选择安装）npm install hexo-generator-feed --save // 建立 RSS 订阅npm install hexo-generator-sitemap --save // 建立站点地图 五、创建ssh （1）打开git bash，在用户主目录下运行：ssh-keygen -t rsa -C “你的邮箱” 把其中的邮件地址换成自己的邮件地址，然后一路回车 （2）最后完成后，会在用户主目录下生成.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH key密钥对，id_rsa是私钥，千万不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 （3）登陆GitHub，打开「Settings」-&gt;「SSH and GPG keys」，然后点击「new SSH key」，填上任意Title，在Key文本框里粘贴公钥id_rsa.pub文件的内容（千万不要粘贴成私钥了！），最后点击「Add SSH Key」，你就应该看到已经添加的Key。 注意：不要在git版本库中运行ssh，然后又将它提交，这样就把密码泄露出去了。 六、部署可能出现的问题 git上传包提交时出现：Please tell me who you are. 解决方法：执行 git config --global user.email &quot;你的邮箱 git config --global user.name “gihub用户名” 之后就可以正常使用啦]]></content>
      <categories>
        <category>博客维护</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F04%2F29%2Fknn%2F</url>
    <content type="text"><![CDATA[title: knn应用（癌症判断） date: 2020-02-03 15:54:50 categories: 机器学习 tags: [KNN,应用，参数] cover: /img/me.jpg KNN简介 KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为： 1）计算测试数据与各个训练数据之间的距离； 2）按照距离的递增关系进行排序； 3）选取距离最小的K个点； 4）确定前K个点所在类别的出现频率； 5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。 距离矩阵和k都是超参数 参数与超参数 参数(parameters)/模型参数 由模型通过学习得到的变量，比如权重和偏置 超参数(hyperparameters)/算法参数 根据经验进行设定，影响到权重和偏置的大小，比如迭代次数、隐藏层的层数、每层神经元的个数、学习速率等 实战 属性信息： Sample code number: id number Clump Thickness: 1 - 10 Uniformity of Cell Size: 1 - 10 Uniformity of Cell Shape: 1 - 10 Marginal Adhesion: 1 - 10 Single Epithelial Cell Size: 1 - 10 Bare Nuclei: 1 - 10 Bland Chromatin: 1 - 10 Normal Nucleoli: 1 - 10 Mitoses: 1 - 10 Class: (2 for benign, 4 for malignant)（2为良性，4为恶性） 为数据添加label，在第一行加入id,clump_thickness,uniform_cell_size, uniform_cell_shape,marginal_adhesion, single_epi_cell_size,bare_nuclei,bland_chromation, normal_nucleoli,mitoses,class 数据样式： clumb_thickness unif_cell_size unif_cell_shape marg_adhesion single_epith_cell_size bare_nuclei bland_chrom norm_nucleoli mitoses class 0 5 1 1 1 2 1 3 1 1 2 1 5 4 4 5 7 10 3 2 1 2 2 3 1 1 1 2 2 3 1 1 2 3 6 8 8 1 3 4 3 7 1 2 4 4 1 1 3 2 1 3 1 1 2 python12345678910111213141516171819202122232425262728293031import numpy as npfrom sklearn import preprocessing ,model_selection,neighborsfrom sklearn.linear_model import LinearRegressionimport pandas as pddf = pd.read_csv('breast-cancer-wisconsin.txt')#encoding='utf-8',header=None,sep = '\t'df.replace('?',-99999, inplace=True)#print([column for column in df])df.drop(['id'], 1, inplace=True) #df.drop returns a new dataframe with our chosen column(s) dropped.#df=df.iloc[:,1:]#.iloc使用全是以0开头的行号和列号，不能直接用其它索引哦。而.loc使用的实际设置的索引和列名。 这就是.loc和.iloc的区别。在实际运用中，我还发现一点区别，.iloc只能选取数据表里实际有的行和列，而.loc可以选取没有的行和列，赋值后就可以添加新行或者列。X = np.array(df.drop(['class'],1))y = np.array(df['class'])X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)#print([column for column in df])#clf = LinearRegression(n_jobs=-1)#SVM.svr() kernel='poly'clf = neighbors.KNeighborsClassifier()clf.fit(X_train,y_train)accuracy = clf.score(X_test,y_test)print(accuracy)print(df.head())#example_measures = np.array([4,2,1,1,1,2,3,2,1]) #一个sample#example_measures = example_measures.reshape(1, -1) #Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sampleexample_measures = np.array([[4,2,1,1,1,2,3,2,1],[4,2,1,1,1,2,3,2,1]])#测试用例example_measures = example_measures.reshape(len(example_measures), -1)prediction = clf.predict(example_measures)print(prediction) accuracy:0.9714285714285714 预测结果：prediction[2 2] 两个都为良性]]></content>
  </entry>
  <entry>
    <title><![CDATA[并行矩阵求逆]]></title>
    <url>%2F2020%2F04%2F25%2Fparallow%2F</url>
    <content type="text"><![CDATA[引言 parallel parallel表示其后语句将被多个线程并行执行，“#pragma omp parallel”后面的语句（或者，语句块）被称为parallel region。 多个线程的执行顺序是不能保证的。 for 我们一般是对一个计算量庞大的任务进行划分，让多个线程分别执行计算任务的某一部分，从而达到缩短计算时间的目的。这里的关键是，每个线程执行的计算互不相同（操作的数据不同或者计算任务本身不同），多个线程协作完成所有计算。 OpenMP for指示将C++ for循环的多次迭代划分给多个线程（划分指，每个线程执行的迭代互不重复，所有线程的迭代并起来正好是C++ for循环的所有迭代），这里C++ for循环需要一些限制从而能在执行C++ for之前确定循环次数，例如C++ for中不应含有break等。 测试下电脑是几核的(几线程) c++123456789101112131415161718#include&lt;omp.h&gt; #include&lt;iostream&gt; int main() &#123; std::cout &lt;&lt; "parallel begin:\n"; #pragma omp parallel &#123; std::cout &lt;&lt; omp_get_thread_num(); &#125; std::cout &lt;&lt; "\n parallel end.\n"; std::cin.get(); return 0; &#125; 参考文献：https://blog.csdn.net/laobai1015/article/details/79020128 问题分析 矩阵求逆大致有三个方法，待定系数法、伴随矩阵求逆矩阵，初等变换求逆矩阵。而待定系数法和伴随矩阵对维数大很难计算了，而初等变化法有着清晰的过程，比较容易用编程语言表达，并且遍历矩阵去操作归一化清零等过程可以很容易实现并行化，不同线程的操作是针对不同行和列也不会产生冲突导致错误。 实现方案： 使用高斯消元法，用二维vector来存储矩阵，方便实现矩阵维度的变化以及遍历，将可并行化的循环加上#pragma omp parallel for实现并行化。 代码思路 运行加速比和正确性验证 代码基于c++和openmp编写，需要代码邮件call我~]]></content>
      <categories>
        <category>并行计算</category>
      </categories>
      <tags>
        <tag>openmp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pachong]]></title>
    <url>%2F2020%2F04%2F21%2Fpachong%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2020%2F04%2F16%2Fpytorch%2F</url>
    <content type="text"><![CDATA[深度学习有许多应用，这些应用往往包括以某种形式获取数据（例如图像或文本），并以另一种形式生成数据（例如标签，数字或更多文本）。从这个角度来看，深度学习包括构建一个将数据从一种表示转换为另一种表示的系统。 从一种数据形式到另一种数据形式的转换通常是由深度神经网络分层次学习的，这意味着我们可以将层次之间转换得到的数据视为一系列中间表示（intermediate representation）。以图像识别为例，浅层的表示可以是特征（例如边缘检测）或纹理（例如毛发），较深层次的表征可以捕获更复杂的结构（例如耳朵、鼻子或眼睛）。 张量（tensor） 对于来自数学、物理学或工程学的人来说，张量一词是与空间、参考系以及它们之间的转换的概念是捆绑在一起的。对于其他人来说，张量是指将向量（vector）和矩阵（matrix）推广到任意维度，。与张量相同概念的另一个名称是多维数组（multidimensional array）。张量的维数与用来索引张量中某个标量值的索引数一致。 张量的优点 Python列表或数字元组（tuple）是在内存中单独分配的Python对象的集合，如图2.3左侧所示。然而，PyTorch张量或NumPy数组（通常）是连续内存块上的视图（view），这些内存块存有未封装（unboxed）的C数值类型，在本例中，如图2.3右侧所示，就是32位的浮点数（4字节），而不是Python对象。因此，包含100万个浮点数的一维张量需要400万个连续字节存储空间，再加上存放元数据（尺寸、数据类型等）的少量开销。 比如可以用zeros或ones来初始化张量，同时用元组指定大小 python12points = torch.zeros(3, 2)points 输出： python123tensor([[0., 0.], [0., 0.], [0., 0.]]) 函数名后面带下划线_ 的函数会修改Tensor本身，例如，x.add_(y)和x.t_()会改变 x，但x.add(y)和x.t()返回一个新的Tensor， 而x不变。 Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[trie]]></title>
    <url>%2F2020%2F03%2F28%2Ftrie%2F</url>
    <content type="text"><![CDATA[python1234567class Solution(object): def minimumLengthEncoding(self, words): res=set(words) for word in words: for k in range(1,len(word)): res.discard(word[k:]) return sum(len(word) + 1 for word in res)]]></content>
  </entry>
  <entry>
    <title><![CDATA[kmeans]]></title>
    <url>%2F2020%2F03%2F25%2Fkmeans%2F</url>
    <content type="text"><![CDATA[简介 k-means algorithm算法是一个聚类算法，把n的对象根据他们的属性分为k个分割，k &lt; n。假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。通过迭代的方式将样本分到K个簇。 基本方法 选取K个点做为初始聚集的簇心（也可选择非样本点）; 分别计算每个样本点到 K个簇核心的距离（这里的距离一般取欧氏距离或余弦距离），找到离该点最近的簇核心，将它归属到对应的簇； 所有点都归属到簇之后， M个点就分为了 K个簇。之后重新计算每个簇的重心（平均距离中心），将其定为新的“簇核心”； 反复迭代 2 - 3 步骤，直到达到某个中止条件 sklearn实现 python1234567891011121314151617181920212223242526import matplotlib.pyplot as pltfrom sklearn.datasets import make_blobsfrom sklearn.cluster import KMeans#产生数据k=4X,Y = make_blobs(n_samples=100, n_features=2, centers=k)#构建模型km = KMeans(n_clusters=k, init='k-means++', max_iter=300)km.fit(X)# 获取簇心centroids = km.cluster_centers_# 获取归集后的样本所属簇对应值y_kmean = km.predict(X)print(y_kmean)# 呈现未归集前的数据plt.scatter(X[:, 0], X[:, 1], s=50)plt.yticks(())plt.show()plt.scatter(X[:, 0], X[:, 1], c=y_kmean, s=50, cmap='viridis')plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)plt.show() 手工实现 python12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npfrom sklearn.datasets import make_blobsfrom math import sqrtimport randomimport matplotlib.pyplot as pltfrom scipy import spatial#产生数据k=4X,Y = make_blobs(n_samples=100, n_features=2, centers=k)def calcuDistance(vec1, vec2): # 步骤1：定义欧式距离的公式 # 计算两个向量之间的欧式距离：根号下[(x_1-x_2)^2+(y_1-y_2)^2+...+(x_n-y_n)^2] # ver1 - ver2：表示两个向量的对应元素相减 return np.sqrt(np.sum(np.square(vec1 - vec2))) #注意这里的减号def k_means(data,k,Y): m, n = data.shape # m：样本数量，n：每个样本的属性值个数 cores = data[np.random.choice(np.arange(m), k, replace=False)] # 从m个数据样本中不重复地随机选择k个样本作为质心 print(cores) while True: # 迭代计算 #d = np.square(np.repeat(data, k, axis=0).reshape(m, k, n) - cores) #distance = np.sqrt(np.sum(d, axis=2)) # ndarray(m, k)，每个样本距离k个质心的距离，共有m行 distance = spatial.distance.cdist(data, cores,metric='euclidean') index_min = np.argmin(distance, axis=1) # 每个样本距离最近的质心索引序号 if (index_min == Y).all(): # 如果样本聚类没有改变 return Y, cores # 则返回聚类结果和质心数据 Y[:] = index_min # 重新分类 for i in range(k): # 遍历质心集 items = Y==i # 找出对应当前质心的子样本集 ，对应的items为[True,false......] cores[i] = np.mean(data[items], axis=0) # 以子样本集的均值作为当前质心的位置 result,cores=k_means(X,k,Y)plt.scatter(X[:, 0], X[:, 1], s=50)plt.yticks(())plt.show()print(Y)plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='viridis')plt.scatter(cores[:, 0], cores[:, 1], c='black', s=100, alpha=0.5)plt.show() k-means的改进 k-means改进的一个路线就是尽可能加快收敛速度，这个方向有几个思路： 1.质心初始化：选择初始质心之间有一些策略比如尽量远离，有助于反应数据的分布，加快收敛。 2.改进k-means的迭代过程，有几个方向，一个改进复杂度，比如数据的访问用KD树来索引，一个是改进目标函数（原始目标函数就是使同一类的离质心距离最小），有一个思路是时刻更新质心，比如移动一个样本到最近的类别，就立刻更新相应的两个类质心，这样改变了每轮都要对所有样本更新label的繁琐过程。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es]]></title>
    <url>%2F2020%2F03%2F23%2Fes%2F</url>
    <content type="text"><![CDATA[开启外网访问的方法：对elasticsearch.yml文件修改下面参数 transport.host: localhost network.host: 192.168.3.5]]></content>
  </entry>
  <entry>
    <title><![CDATA[list]]></title>
    <url>%2F2020%2F03%2F20%2Flist%2F</url>
    <content type="text"><![CDATA[移除链表元素 一道简单题卡了很久…太长时间不做链表果然忘记太多，下面总结一下几点错误 leetcode的链表都没有头节点，head指针直接指向第一个元素，所以如果想要删除第一个的元素的话需要自己建立一个头节点 开始代码写得是return head，如果第一个元素被删除了，那么head后面的指向就断掉了 最开始循环里是这么写的 c++1234 if(p-&gt;next-&gt;val==val)&#123; p-&gt;next=p-&gt;next-&gt;next; p=p-&gt;next;&#125; 这样的话删除后就会跳过一个元素了。 下面代码， 常规 c++12345678910111213141516class Solution &#123;public: ListNode* removeElements(ListNode* head, int val) &#123; if(head==NULL)return NULL; ListNode*q = new ListNode(-1); q-&gt;next = head; ListNode*p = q; while(p-&gt;next!=NULL)&#123; if(p-&gt;next-&gt;val==val) p-&gt;next=p-&gt;next-&gt;next; else p=p-&gt;next; &#125; return q-&gt;next; &#125;&#125;; 递归 c++123456789class Solution &#123;public: ListNode* removeElements(ListNode* head, int val) &#123; if (!head) return head; head-&gt;next = removeElements(head-&gt;next, val); return head-&gt;val == val ? head-&gt;next : head; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程(持续更新)]]></title>
    <url>%2F2020%2F03%2F14%2Fpython%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/ImwaterP/article/details/96282230 argmin 该函数主要用来检索数组中最小值的位置，并返回其下标值。同理，argmax()函数就是用来检索最大值的下标 在没有指定axis值的情况下，默认为None。在默认情况下，就相当于将n维的arry平铺在一起。举个简单的例子，当二维arry（[1,2,3],[4,5,6]）平铺开来就是（[1,2,3,4,5,6]）。 当axis = 1时，按照方向来，对于[2,5,6]来说最小值的下标是0，对于[7,6,1]来说最小值的下标是2。所以，最后输出的值就是[0,2]。 当axis = 0时，这时按照方向来，[2,7],[5,6],[6,1]分别在一个轴上，所以检索每个轴上的最小值，并返回下标，最后就可以得到输出值[0,0,1]。 关于切片 在list里面，只存在元素，不存在元素中的元素；list里元素就是最小的成分，不可以再切片。numpy 的array可以切片 例如 python1234a=np.array([[1,5],[2,6],[3,7]])print(a[2]) print(a[2,:])都可以a=[[1,5],[2,6],[3,7]]print(a[2,:])会报错 scipy.spatial.distance.cdist 该函数用于计算两个输入集合的距离，通过metric参数指定计算距离的不同方式得到不同的距离度量值 https://blog.csdn.net/kancy110/article/details/75675574]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>pyton</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2020%2F03%2F12%2Flinux%2F</url>
    <content type="text"><![CDATA[共享文件夹there no need 现在本地建立一个文件夹，这里是F:\test然后点击设备–共享文件夹，勾选自动挂载，自动分配， 此处我选择在/mnt下创建一个“share”目录，将刚刚的“gongxiang”目录与“share”目录关联起来。 进入/mnt： Code1cd/mnt 创建share目录： Code1sudo makdir share 将“test”目录与“share”目录进行关联： Code1sudo mount -t vboxsf test /mnt/share 报错对omp_getnum_threads未定义的引用 https://stackoverflow.com/questions/9685377/undefined-reference-to-omp-get-max-threads https://stackoverflow.com/questions/9685377/undefined-reference-to-omp-get-max-threads #更改分辨率 先使用xrandr命令看下分辨率的索引 然后xrandr --size 00(这是800800的)]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[say somethingto myself]]></title>
    <url>%2F2020%2F03%2F10%2Ffor-me%2F</url>
    <content type="text"><![CDATA[该文章已加密, 请输入密码查看。 38cd76bef1cf79f27267863aa0f1e10aff0a55b3032fb665653565d974dcbbb6fb9e335fcb6f2cac947609853f0c56f0dbf7300ebc6e47358c153341a4c18fe59abedea6571c7603dd0c3ebeb08faededad4174f6c6eeda32fe389116f16e28f0210187cfa4a272439508fc90658c1b2309a116992d303d700f05971b0e4855d4f4ff589cfc7838359191bd9ee6077a648cc1789b19943078668c653d9694c0c484aaaa3e126d8a153c6e91c689217e97fdcb7997cda884612bdc08273526ce2ae6489c37d097434752f4027a4af6877]]></content>
  </entry>
  <entry>
    <title><![CDATA[gragh_convolution]]></title>
    <url>%2F2020%2F03%2F08%2Fgragh-convolution%2F</url>
    <content type="text"><![CDATA[graghsage 论文链接：https://arxiv.org/abs/1706.02216 github链接：https://github.com/williamleif/GraphSAGE 官方介绍链接：http://snap.stanford.edu/graphsage/ 优秀介绍： https://blog.csdn.net/yyl424525/article/details/100532849?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task gcn 现存的方法需要图中所有的顶点在训练embedding的时候都出现；这些前人的方法本质上是transductive，不能自然地泛化到未见过的顶点。 GraphSAGE是为了学习一种节点表示方法，即如何通过从一个顶点的局部邻居采样并聚合顶点特征，而不是为每个顶点训练单独的embedding。 GCN虽然能提取图中顶点的embedding，但是存在一些问题： GCN的基本思想： 把一个节点在图中的高纬度邻接信息降维到一个低维的向量表示。 GCN的优点： 可以捕捉graph的全局信息，从而很好地表示node的特征。 GCN的缺点： Transductive learning的方式，需要把所有节点都参与训练才能得到node embedding，无法快速得到新node的embedding。 GCN等transductive的方法，学到的是每个节点的一个唯一确定的embedding； 而GraphSAGE方法学到的node embedding，是根据node的邻居关系的变化而变化的，也就是说，即使是旧的node，如果建立了一些新的link，那么其对应的embedding也会变化，而且也很方便地学到。 算法概述 KKK:K是网络的层数，也代表着每个顶点能够聚合的邻接点的跳数，如K=2的时候每个顶点可以最多根据其2跳邻接点的信息学习其自身的embedding表示。每增加一层可以聚合更远节点的信息 N(v)N_{(v)}N(v)​:GraphSAGE中每一层的节点邻居都是是从上一层网络采样的，并不是所有邻居参与，并且采样的后的邻居的size是固定的 其运行流程如上图所示，可以分为三个步骤： 对图中每个顶点邻居顶点进行采样，因为每个节点的度是不一致的，为了计算高效， 为每个节点采样固定数量的邻居 根据聚合函数聚合邻居顶点蕴含的信息 得到图中各顶点的向量表示供下游任务使用 Neighborhood definition - 采样邻居顶点 出于对计算效率的考虑，对每个顶点采样一定数量的邻居顶点作为待聚合信息的顶点。设需要的邻居数量，即采样数量为SSS，若顶点邻居数少于SSS,则采用有放回的抽样方法，直到采样出SSS个顶点。若顶点邻居数大于SSS，则采用无放回的抽样。(即采用有放回的重采样/负采样方法达到SSS) 当然，若不考虑计算效率，完全可以对每个顶点利用其所有的邻居顶点进行信息聚合，这样是信息无损的。 文中在较大的数据集上实验。因此，统一采样一个固定大小的邻域集，以保持每个batch的计算占用空间是固定的（即 graphSAGE并不是使用全部的相邻节点，而是做了固定size的采样）。 这样固定size的采样，每个节点和采样后的邻居的个数都相同，可以把每个节点和它们的邻居拼成一个batch送到GPU中进行批训练。 论文里说固定长度的随机游走其实就是随机选择了固定数量的邻居 聚合函数的选取## 在图中顶点的邻居是无序的，所以希望构造出的聚合函数是对称的（即也就是对它输入的各种排列，函数的输出结果不变），同时具有较高的表达能力。 聚合函数的对称性（symmetry property）确保了神经网络模型可以被训练且可以应用于任意顺序的顶点邻居特征集合上。 主要有mean embedding，LSTM,pooling 代码理解]]></content>
      <categories>
        <category>图</category>
      </categories>
      <tags>
        <tag>graghsage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2F2020%2F03%2F06%2Fskipgram%2F</url>
    <content type="text"><![CDATA[自然语言模型的发展与引出 https://www.cnblogs.com/guoyaohua/p/9240336.html 基于频率或者预测模型：https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/ 语言模型会给一个正常的语句很高的概率 unigram:$$P(w_{1},w_{2}…w_{n})=\prod_{i=1}^{n}P(w_{i})$$ 然而下一个词很大程度会取决于前面序列的词，这样独立概率会让一些愚蠢的语句也可以得到很大的值，所以引出了 Bigram model:$$P(w_{1},w_{2}…w_{n})=\prod_{i=2}^{n}P(w_{i}|w_{i-1})$$ 模型解析 概念 语料(corpus)是指文本所有内容，包括重复的词，词典DDD是从语料中抽取出来的不包括重复词语 one-hot这种表示方式使得每一个词映射到高维空间中都是互相正交的，也就是说one-hot向量空间中词与词之间没有任何关联关系，这显然与实际情况不符合，因为实际中词与词之间有近义、反义等多种关系。Word2vec虽然学习不到反义这种高层次语义信息，但它巧妙的运用了一种思想：“具有相同上下文的词语包含相似的语义”，使得语义相近的词在映射到欧式空间后中具有较高的余弦相似度 其实总体思想还是降维，one-hot表达维度太大了，svd矩阵分解两个低纬度矩阵。 权重矩阵 https://blog.csdn.net/itplus/article/details/37969979 以skipgram为例主要有两个权重矩阵,第一个是中心词的向量表达矩阵VVV,第二个是上下文单词的向量表达矩阵UUU,他们都是D×VD\times VD×V维的 下面总结一下计算过程: 1.首先输入中心词ωt\omega_{t}ωt​的one-hot编码(V×1V\times 1V×1) 2.接着与矩阵VVV运算得到中心词的representation vc=ωt⋅Vv_{c}=\omega_{t}\cdot Vvc​=ωt​⋅V (D×1D\times1D×1) 3.下一步就是中心词向量vcv_{c}vc​与矩阵UUU相乘（u0Tvcu_{0}^{T}v_{c}u0T​vc​,u0u_{0}u0​就是矩阵UUU的某一行，其实这个就是即某个上下文词的one-hot的表达乘以UUU得到representation,u0Tvcu_{0}^{T}v_{c}u0T​vc​这最后得到的就是V×1V\times 1V×1向量的一维) 可以听cs24n的视频讲解 4. 下面用softmax把相似性大小转变为概率 详细的一个例子：https://cloud.tencent.com/developer/article/1591734 输出层 对应一颗二叉树，词典中的词作为叶子节点，根据单词出现次数作为权值，构造huffman树，叶子节点一共∣D∣|D|∣D∣个,每一次分支都是二分类，分到左面是负类，右面是正类，详细过程,https://www.cnblogs.com/neopenx/p/4571996.html 代码 源代码 #构造一个神经网络，输入词语，输出词向量 python123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166import numpy as npimport torchfrom torch import nn, optimimport randomfrom collections import Counterimport matplotlib.pyplot as plt#训练数据#text = "I like dog i like cat i like animal dog cat animal apple cat dog like dog fish milk like dog \#cat eyes like i like apple apple i hate apple i movie book music like cat dog hate cat dog like"with open('/content/text8') as f: #colab上的路径 text = f.read()#参数设置EMBEDDING_DIM = 2 #词向量维度PRINT_EVERY = 1000 #可视化频率EPOCHS = 3 #训练的轮数BATCH_SIZE = 5 #每一批训练数据大小N_SAMPLES = 3 #负样本大小WINDOW_SIZE = 5 #周边词窗口大小FREQ = 0 #词汇出现频率DELETE_WORDS = False #是否删除部分高频词#文本预处理def preprocess(text, FREQ): text = text.lower() words = text.split() #去除低频词 word_counts = Counter(words) trimmed_words = [word for word in words if word_counts[word] &gt; FREQ] return trimmed_wordswords = preprocess(text, FREQ)#构建词典vocab = set(words)vocab2int = &#123;w: c for c, w in enumerate(vocab)&#125;int2vocab = &#123;c: w for c, w in enumerate(vocab)&#125;#将文本转化为数值int_words = [vocab2int[w] for w in words]#计算单词频次int_word_counts = Counter(int_words)total_count = len(int_words)word_freqs = &#123;w: c/total_count for w, c in int_word_counts.items()&#125;#items()方法把字典中每对key和value组成一个元组，并把这些元组放在列表中返回。#去除出现频次高的词汇if DELETE_WORDS: t = 1e-5 prob_drop = &#123;w: 1-np.sqrt(t/word_freqs[w]) for w in int_word_counts&#125; train_words = [w for w in int_words if random.random()&lt;(1-prob_drop[w])]else: train_words = int_words#单词分布word_freqs = np.array(list(word_freqs.values()))unigram_dist = word_freqs / word_freqs.sum()noise_dist = torch.from_numpy(unigram_dist ** (0.75) / np.sum(unigram_dist ** (0.75)))#获取目标词汇def get_target(words, idx, WINDOW_SIZE): target_window = np.random.randint(1, WINDOW_SIZE+1) start_point = idx-target_window if (idx-target_window)&gt;0 else 0 end_point = idx+target_window targets = set(words[start_point:idx]+words[idx+1:end_point+1]) return list(targets)#批次化数据def get_batch(words, BATCH_SIZE, WINDOW_SIZE): n_batches = len(words)//BATCH_SIZE words = words[:n_batches*BATCH_SIZE] for idx in range(0, len(words), BATCH_SIZE): batch_x, batch_y = [],[] batch = words[idx:idx+BATCH_SIZE] for i in range(len(batch)): x = batch[i] y = get_target(batch, i, WINDOW_SIZE) batch_x.extend([x]*len(y)) batch_y.extend(y) yield batch_x, batch_y#定义模型class SkipGramNeg(nn.Module): def __init__(self, n_vocab, n_embed, noise_dist): super().__init__() self.n_vocab = n_vocab self.n_embed = n_embed self.noise_dist = noise_dist #定义词向量层 self.in_embed = nn.Embedding(n_vocab, n_embed) self.out_embed = nn.Embedding(n_vocab, n_embed) #词向量层参数初始化 self.in_embed.weight.data.uniform_(-1, 1) self.out_embed.weight.data.uniform_(-1, 1) #输入词的前向过程 def forward_input(self, input_words): input_vectors = self.in_embed(input_words) return input_vectors #目标词的前向过程 def forward_output(self, output_words): output_vectors = self.out_embed(output_words) return output_vectors #负样本词的前向过程 def forward_noise(self, size, N_SAMPLES): noise_dist = self.noise_dist #从词汇分布中采样负样本 noise_words = torch.multinomial(noise_dist, size * N_SAMPLES, replacement=True) noise_vectors = self.out_embed(noise_words).view(size, N_SAMPLES, self.n_embed) return noise_vectors#定义损失函数class NegativeSamplingLoss(nn.Module): def __init__(self): super().__init__() def forward(self, input_vectors, output_vectors, noise_vectors): BATCH_SIZE, embed_size = input_vectors.shape #将输入词向量与目标词向量作维度转化处理 input_vectors = input_vectors.view(BATCH_SIZE, embed_size, 1) output_vectors = output_vectors.view(BATCH_SIZE, 1, embed_size) #目标词损失 test = torch.bmm(output_vectors, input_vectors) out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log() out_loss = out_loss.squeeze() #负样本损失 noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log() noise_loss = noise_loss.squeeze().sum(1) #综合计算两类损失 return -(out_loss + noise_loss).mean()#模型、损失函数及优化器初始化model = SkipGramNeg(len(vocab2int), EMBEDDING_DIM, noise_dist=noise_dist)criterion = NegativeSamplingLoss()optimizer = optim.Adam(model.parameters(), lr=0.003)#训练steps = 0for e in range(EPOCHS): #获取输入词以及目标词 for input_words, target_words in get_batch(train_words, BATCH_SIZE, WINDOW_SIZE): steps += 1 inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words) #输入、输出以及负样本向量 input_vectors = model.forward_input(inputs) output_vectors = model.forward_output(targets) size, _ = input_vectors.shape noise_vectors = model.forward_noise(size, N_SAMPLES) #计算损失 loss = criterion(input_vectors, output_vectors, noise_vectors) #打印损失 if steps%PRINT_EVERY == 0: print("loss：",loss) #梯度回传 optimizer.zero_grad() loss.backward() optimizer.step()#可视化词向量for i, w in int2vocab.items() : vectors = model.state_dict()["in_embed.weight"] x,y = float(vectors[i][0]),float(vectors[i][1]) plt.scatter(x,y) plt.annotate(w, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')plt.show() 评估词向量 可视化 相似度计算（余弦） analogy类比]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode_day]]></title>
    <url>%2F2020%2F03%2F02%2Fleetcode-day%2F</url>
    <content type="text"><![CDATA[3.1 3.2(链表，迭代，递归) 类似于头插法 c++1234567891011121314151617class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if(head==NULL) return NULL; ListNode* pre=NULL; ListNode* cur=head; while(cur!=NULL) &#123; ListNode* p=cur-&gt;next; cur-&gt;next= pre; pre=cur; cur=p; &#125; return pre; &#125;&#125;; 递归：关键就是理解p是反转后链表的表头 c++12345678910111213class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if (head == NULL || head-&gt;next == NULL) return head; // 如果当前要反转的节点为 null 或者反转链表为 null // head.next 为 null，即反转链表的尾结点不存在，即反转链表不存在 ListNode *p = reverseList(head-&gt;next);// 节点 p 其实就是反转链表的头节点 head-&gt;next-&gt;next = head; head-&gt;next = NULL; return p; &#125;&#125;; 3.3(数组，双指针) c++12345678910111213141516class Solution &#123;public: void merge(vector&lt;int&gt;&amp; A, int m, vector&lt;int&gt;&amp; B, int n) &#123; int i=m-1; int j=n-1; int index=m+n-1; while(i&gt;=0&amp;&amp;j&gt;=0) &#123; if(B[j]&gt;=A[i]) A[index--]=B[j--]; else A[index--]=A[i--]; &#125; while(j &gt;= 0) A[index--] = B[j--]; &#125;&#125;;]]></content>
  </entry>
  <entry>
    <title><![CDATA[nlp]]></title>
    <url>%2F2020%2F02%2F28%2Fnlp%2F</url>
    <content type="text"><![CDATA[RNN 如果用标准神经网络来处理序列的话，因为词向量用one hot表示会有很大的维数，这是很庞大的输入层，第一层权重矩阵就会有很多参数 RNN公式：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文整理]]></title>
    <url>%2F2020%2F02%2F25%2Fpaper1%2F</url>
    <content type="text"><![CDATA[论文整理： truss： Efficient Truss Maintenance in Evolving Networks(2014) 1证明了每次插入边后，truss最多加1 2.插入边后，受影响truss的范围 Streaming and Batch Algorithms for Truss(2019) 插入一条边后对不同k值边的更新不会互相影响。 Decomposition core K-core Minimization: An Edge Manipulation Approach(2018) 因为m条里选出b个组合复杂度过高，提出了两个greedy算法，第一个就是对k-core里的每条边删除计算followers，选出b个最优解。第二个算法是对候选集和的优化，只选择k-core图中两个顶点分别为k和大于k的边。]]></content>
      <tags>
        <tag>truss</tag>
        <tag>core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新冠状病毒预测]]></title>
    <url>%2F2020%2F02%2F25%2Fvirus%2F</url>
    <content type="text"><![CDATA[闲来无事，做一个确诊人数的预测吧，希望拐点早日降临。 首先数据就是日期和总确诊人数，走势是平缓到爆发到平缓，所以用logistics函数。总治愈目前处于上升趋势，多项式拟合吧。大体思路就是自变量特征从1到总的天数，然后把数字映射到日期，制图。 附上代码 python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as npimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom scipy.optimize import curve_fitdef date_encode(date): # '01.24' -&gt; 1 * 100 + 24 = 124 d = date.split('/') return int(d[1]),int(d[2])def date_decode(date): # 124 -&gt; '01.24' return '&#123;&#125;.&#123;&#125;'.format(str(date // 100), str(date % 100))df = pd.read_csv('data.csv')#encoding='utf-8',header=None,sep = '\t'df.drop([33],inplace=True)X = np.array(df.iloc[:,0]) #日期太多显示会重叠，前十天数据很平缓故先忽略掉cur_month,cur_day=date_encode(X[0])y = np.array(df['total_confirmed'])z= np.array(df['new_recoveries'])x = np.arange(len(y))def get_date_list(cur_month,cur_day,days,prediction=7): """ 得到原始数据和预测的日期 """ month_day = [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] ans = [] n=days+prediction while n: if cur_day &lt;= month_day[cur_month]: d = "0" + str(cur_day) if cur_day &lt; 10 else str(cur_day) ans += [str(cur_month) + "/" + d] cur_day += 1 n=n-1 else: cur_day = 1 cur_month += 1 n=n-1 return ansans = get_date_list(cur_month,cur_day,len(y),prediction=7)def logistic_function(t, K, P0, r): r=0.27 t0 = 0 exp = np.exp(r * (t - t0)) return (K * exp * P0) / (K + (exp - 1) * P0)def f_3(x, A, B, C, D): return A*x*x*x + B*x*x + C*x + Dpopt, pcov = curve_fit(logistic_function, x, y)popt1, pcov1 = curve_fit(f_3, x, z)predict_x = list(x)+[x[-1] + i for i in range(1, 8)] #数组合并不能直接相加predict_x = np.array(predict_x)predict_y = logistic_function(predict_x, popt[0], popt[1], popt[2])predict_y = [int(i) for i in predict_y]predict_z = f_3(predict_x, popt1[0], popt1[1], popt1[2],popt1[3])predict_z = [int(i) for i in predict_z]#print(ans[-7:],predict_y[-7:])#输出新增确诊new_infected = [predict_y[i]-predict_y[i-1] for i in range(-7,0)] print(ans[-7:],new_infected)#plt.scatter(x,y,color='purple',label='real')#plt.plot(x,y,color='gray')#plt.scatter(predict_x,predict_y,marker='x',color='red',label='predicted data')#plt.xticks(predict_x,ans,rotation=90)#plt.suptitle("Logistic Fitting Curve for 2019-nCov total infected numbers", fontsize=16, fontweight="bold")#输出新增治愈new_cured = [predict_z[i]-predict_z[i-1] for i in range(-7,0)] print(ans[-7:],new_cured)plt.scatter(x,z,color='purple',label='real')plt.plot(x,z,color='gray')plt.scatter(predict_x,predict_z,marker='x',color='red',label='predicted data')plt.xticks(predict_x,ans,rotation=90)plt.suptitle("polynomial regression Fitting Curve for 2019-nCov total cured numbers", fontsize=16, fontweight="bold")plt.xlabel('date', fontsize=14)plt.ylabel('infected number', fontsize=14)plt.show() 预测走势 新增确诊人数： 新增治愈趋势： 新增治愈人数： 导出csv文件 导出文件，list作为列的方法： python123456a = ['2020/2/24', '2020/2/25', '2020/2/26', '2020/2/27', '2020/2/28', '2020/2/29', '2020/3/01']b = [307, 236, 181, 139, 106, 81, 63]c = [2818, 3019, 3229, 3447, 3673, 3909, 4153]data = &#123;'date':a,'new_confirmed':b,'new_recoveries':c&#125;dataframe = pd.DataFrame(data)dataframe.to_csv(r'D:\python\test.csv') 结果 用list导入dataframe是list[[行],[行]] python1234567list=[new_infected[i] for i in range(-7,0)]list1=[predict_z[i] for i in range(-7,0)]list.extend(list1)list=[list]column=['confirmed_day1','confirmed_day2','confirmed_day3','confirmed_day4','confirmed_day5','confirmed_day6','confirmed_day7','recovery_day1','recovery_day2','recovery_day3','recovery_day4','recovery_day5','recovery_day6','recovery_day7'] test=pd.DataFrame(columns=column,data=list,index=['total'])test.to_csv('D:/test1.csv')]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>多项式回归</tag>
        <tag>logistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode1326]]></title>
    <url>%2F2020%2F02%2F21%2Fleetcode1326%2F</url>
    <content type="text"><![CDATA[解码方式 首先分析几个特殊的情况，之后开始dp dp[j]对应s[0]到s[i-1]的译码总数 如果s[i]=0,前一位只能是1或2，dp[j]=dp[j-2]; 如果s[i-1]'1’或者s[i-1]‘2’&amp;&amp;s[i]&lt;=‘6’，这两种情况后两位可以合并或者分开译码s[i-1]和s[i]分开译码就是dp[j-1]，合并译码就是dp[j-2] 其他情况就是只能分开译码了 c++123456789101112131415161718192021222324252627class Solution &#123;public: int numDecodings(string s) &#123; if(s.length()==0) return 0; if(s[0]=='0')return 0; if(s.length()==1) return 1; int dp[s.size()+1]; dp[0]=dp[1]=1; for(int i=1,j=2;i&lt;s.size();i++,j++) &#123; if(s[i]=='0') &#123; if(s[i-1]=='1'||s[i-1]=='2') dp[j]=dp[j-2]; else return 0; &#125; else &#123; if(s[i-1]=='1'||s[i-1]=='2'&amp;&amp;s[i]&lt;='6') dp[j]=dp[j-1]+dp[j-2]; else dp[j]=dp[j-1]; &#125; &#125; return dp[s.size()]; &#125;&#125;; 整数拆分 给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。 python1234567891011121314class Solution &#123;public: int integerBreak(int n) &#123; vector&lt;int&gt;dp(n+1,0); dp[1]=1; for(int i=2;i&lt;=n;i++) for(int j=i-1;j&gt;=1;j--) &#123; dp[i]=max(dp[i],dp[j]*(i-j)); dp[i]=max(dp[i],j*(i-j)); &#125; return dp[n]; &#125;&#125;; 357.计算各个位数不同的数字个数 python1234567891011121314class Solution &#123;public: int countNumbersWithUniqueDigits(int n) &#123; if(n==0)return 1; //n=0时，数组长度为1，运行到dp[1]会指向空地址 vector&lt;int&gt;dp(n+1,0); dp[0]=1; dp[1]=10; for(int i=2;i&lt;=n;i++) &#123; dp[i]=dp[i-1]+(dp[i-1]-dp[i-2])*(10-(i-1)); 之前的解加上新增的i位数情况，i位数是由i-1位数加上一位数，再加的一位数只有10-(i-1)种情况。 &#125; return dp[n]; &#125;&#125;; 以n=3为例，n=2已经计算了0-99之间不重复的数字了，我们需要判断的是100-999之间不重复的数字，那也就只能用10-99之间的不重复的数去组成三位数，而不能使用0-9之间的不重复的数，因为他们也组成不了3位数。而10-99之间不重复的数等于dp[2]-dp[1]。 当i=2时，说明之前选取的数字只有1位，那么我们只要与这一位不重复即可，所以其实有9(10-1)种情况（比如1，后面可以跟0,2,3,4,5,6,7,8,9）。当i=3时，说明之前选取的数字有2位，那么我们需要与2位不重复，所以剩余的有8（10-2）种（比如12，后面可以跟0,3,4,5,6,7,8,9） 1326.灌溉花园的最少水龙头数目 c++1234567891011121314151617class Solution &#123;public: int dp[10001]; int INF= 0x3f3f3f3f; //定义为无穷大，意味着无法灌溉 int minTaps(int n, vector&lt;int&gt;&amp; ranges) &#123; memset(dp,INF,sizeof(dp)); dp[0] = 0; for(int i = 0; i &lt; ranges.size(); i++)&#123; int L = max(0,i-ranges[i]); int R = min(n,i+ranges[i]); for(int j = L; j &lt;= R; j++)&#123; dp[j] = min(dp[j],dp[L]+1); // &#125; &#125; return dp[n] == INF ? -1 : dp[n]; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面部识别]]></title>
    <url>%2F2020%2F02%2F20%2Fface%2F</url>
    <content type="text"><![CDATA[one shot]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BFS]]></title>
    <url>%2F2020%2F02%2F13%2FBFS%2F</url>
    <content type="text"><![CDATA[994.腐烂的橘子 腐烂橘子的影响范围是周围一圈的橘子，这就是典型的BFS,类似于拓扑排序，每一轮bfs都记录一下 c++12345678910111213141516171819202122232425262728class Solution &#123;public: int orangesRotting(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int dx[]=&#123;0,0,1,-1&#125;,dy[]=&#123;-1,1,0,0&#125;;//方向数组 int m=grid.size(),n=grid[0].size(); int res=0; queue&lt;pair&lt;int,int&gt;&gt; q; //1、初始化队列：添加烂橘子 for(int i=0;i&lt;m;++i)for(int j=0;j&lt;n;++j)if(grid[i][j]==2)q.push(&#123;i,j&#125;); //2、进行bfs：将每层橘子中四个方向的好橘子感染成烂橘子，并添加到队列中 while(!q.empty())&#123; int span=q.size(); for(int i=0;i&lt;span;++i)&#123; pair&lt;int,int&gt; p=q.front();q.pop(); for(int j=0;j&lt;4;++j)&#123;//将每个烂橘子的4个方向的好橘子感染成烂橘子 int x=p.first+dx[j],y=p.second+dy[j]; if(x&gt;=0&amp;&amp;x&lt;m&amp;&amp;y&gt;=0&amp;&amp;y&lt;n&amp;&amp;grid[x][y]==1)&#123; grid[x][y]=2; q.push(&#123;x,y&#125;); &#125; &#125; &#125; if(!q.empty())res++;//感染完一圈的橘子，res+1 &#125; for(int i=0;i&lt;m;++i)for(int j=0;j&lt;n;++j)if(grid[i][j]==1)return -1; return res; &#125;&#125;; 207.课程表 这道题等价于判断图里有没有环，两种方法一个是拓扑排序，一个是DFS 拓扑排序 c++1234567891011121314151617181920212223242526272829303132class Solution &#123;public: bool canFinish(int numCourses, vector&lt;vector&lt;int&gt;&gt;&amp; prerequisites) &#123; vector&lt;int&gt; degree(numCourses, 0); // map&lt;int,int&gt;degree;for(int i=0;i&lt;numCourses;i++)degree[i]=0; map&lt;int,vector&lt;int&gt;&gt;cur; queue&lt;int&gt;q; for(int i=0;i&lt;prerequisites.size();i++)&#123; cur[prerequisites[i][1]].push_back(prerequisites[i][0]); degree[prerequisites[i][0]]++; &#125; for(int i=0;i&lt;degree.size();i++) //不是cur.size() &#123; if(degree[i]==0) q.push(i); &#125; int ans=0; while(!q.empty()) &#123; int node=q.front(); q.pop(); ans++; for(int i=0;i&lt;cur[node].size();i++) &#123; degree[cur[node][i]]--; if(degree[cur[node][i]]==0) q.push(cur[node][i]); &#125; &#125; if(ans==numCourses)return true; else return false; &#125;&#125;; 在这里我犯了一个错误，找了半天…啊我的时间都去哪了… 对于入度容器我开始设计是C++ map&lt;int,vector&lt;int&gt;&gt;cur这样没有考虑度为0的节点，首先默认节点入度都为0，然后根据图来更新入度，这种情况可以就地改进见注释，或者直接用vector初始化]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep_learning]]></title>
    <url>%2F2020%2F02%2F13%2Fdeep-learning%2F</url>
    <content type="text"><![CDATA[激活函数（Activation functions） 如果不使用激活函数，无论神经网络多少层都会是个线性激活函数 除了二分类问题，不要使用sigmoid，tanh变现总是更好，但这些函数当z很大时，梯度会很小，训练会很慢，所以推荐relu函数。 ReLu函数只要zzz是正值的情况下，导数恒等于1，当是zzz负值的时候，导数恒等于0。从实际上来说，当使用的导数时，zzz=0的导数是没有定义的。但是当编程实现的时候，zzz的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，zzz是等于0的时候，假设一个导数是1或者0效果都可以。这里也有另一个版本的Relu被称为Leaky Relu,这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLu使用的并不多. 对于sigmoid函数g(z)=11+e−xg(z)=\frac{1}{1+e^{-x}}g(z)=1+e−x1​,他的导数等于g(z)×(1−g(z))g(z)\times (1-g(z))g(z)×(1−g(z)) 对于tanh函数ex−e−xex+e−x\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}ex+e−xex−e−x​,他的导数等于1−(tanh(z))21-(tanh(z))^{2}1−(tanh(z))2 Relu函数g(z)=max(0,z)g(z)=max(0,z)g(z)=max(0,z),leaky Relu函数g(z)=max(0.01z,z)g(z)=max(0.01z,z)g(z)=max(0.01z,z) 矩阵的维数 比如图中的神经网络，第一个隐藏层z[1]=w[1]x+b[1]z^{[1]}=w^{[1]}x+b^{[1]}z[1]=w[1]x+b[1],z[1]z^{[1]}z[1]是3×1的矩阵，xxx是2×1的矩阵，所以w[1]w^{[1]}w[1]是3×2的，总结起来就是w[L]w^{[L]}w[L]是n[L]×n[L−1]n^{[L]}\times n^{[L-1]}n[L]×n[L−1]的，对dwdwdw也是一样的，b[L]b^{[L]}b[L]就是n[L]×1n^{[L]}\times1n[L]×1的，见图片右半部分 之后可以将z[1]z^{[1]}z[1]叠加起来，m为样本数量 参数随机初始化（Random+Initialization） W[1]=np.random.rando(2,2)∗0.01,b=np.zeros((2,1))W^{[1]}=np.random.rando(2,2)*0.01,b=np.zeros((2,1))W[1]=np.random.rando(2,2)∗0.01,b=np.zeros((2,1)) 为什么是0.01，而不是100或者1000。我们通常倾向于初始化为很小的随机数。因为如果你用tanh或者sigmoid激活函数，或者说只在输出层有一个Sigmoid，如果WWW很大，ZZZ就会很大或者很小，因此这种情况下你很可能停在tanh/sigmoid函数的平坦的地方(见图3.8.2)，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。 正则化（Regularization） 1.为什么只正则化参数www？为什么不再加上参数bbb呢？你可以这么做，只是我习惯省略不写，因为通常是一个高维参数矢量，已经可以表达高偏差问题，可能包含有很多参数，我们不可能拟合所有参数，而bbb只是单个数字，所以www几乎涵盖所有参数，而不是bbb，如果加了参数bbb，其实也没太大影响，因为只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。 2.为什么正则化会有用？当λ\lambdaλ增大，www接近于0，会减少很多隐藏单元的影响，网络会变得简单，接近于逻辑回归，zzz也会很小(z[l]=w[l]a[l−1]+b[l]z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}z[l]=w[l]a[l−1]+b[l]),会呈线性。 3.其他正则化方法：数据扩增（比如图片翻转、裁剪、扭曲），early stop early stop无法将降低损失函数和过拟合独立处理，会很复杂，所以更倾向L2正则化，虽然要尝试很多不同的λ\lambdaλ，计算代价会很大 归一化 为什么要归一化？代价函数看起来会更对称，无论从哪个位置开始都能更直接的找到最小值，可以使用较大的步长。 padding 普通的卷积两个缺点，第一个缺点是每次做卷积操作，你的图像就会缩小，从6×6缩小到4×4，你可能做了几次之后，你的图像就会变得很小了，可能会缩小到只有1×1的大小。你可不想让你的图像在每次识别边缘或其他特征时都缩小，这就是第一个缺点。第二个缺点时，如果你注意角落边缘的像素，这个像素点只被一个输出所触碰或者使用，因为它位于这个3×3的区域的一角。但如果是在中间的像素点，就会有许多3×3的区域与之重叠。所以那些在角落或者边缘区域的像素点在输出中采用较少，意味着你丢掉了图像边缘位置的许多信息。 对于N×N/N \times N/N×N/的图像，f×ff \times ff×f的filter（通常为奇数），paddy为p，步长stride为2，最后得到的矩阵为[n+2p−fs+1]×[n+2p−fs+1][\frac{n+2p-f}{s}+1] \times [\frac{n+2p-f}{s}+1][sn+2p−f​+1]×[sn+2p−f​+1],如果这个不是整数，我们向下取整，]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[knn_application]]></title>
    <url>%2F2020%2F02%2F10%2Fknn-application%2F</url>
    <content type="text"><![CDATA[python counter类: python12345&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; s = "hello pinsily"&gt;&gt;&gt; d = Counter(s)&gt;&gt;&gt; dCounter(&#123;'l': 3, 'i': 2, 'h': 1, 'e': 1, 'o': 1, ' ': 1, 'p': 1, 'n': 1, 's': 1, 'y': 1&#125;) most_common(n) 返回数量最多的前 n 个元素 python12&gt;&gt;&gt; d.most_common(3)[('l', 3), ('i', 2), ('h', 1)] 代码实现： python12345678910111213141516171819202122232425262728293031import numpy as np from math import sqrtimport matplotlib.pyplot as pltimport warningsfrom matplotlib import stylefrom collections import Counterstyle.use('fivethirtyeight')dataset = &#123;'k':[[1,2],[2,3],[3,1]], 'r':[[6,5],[7,7],[8,6]]&#125;new_features = [5,7]for i in dataset: for ii in dataset[i]: plt.scatter(ii[0],ii[1],s=100,color=i)def k_nearest_neighbors(data, predict, k=3): if len(data) &gt;= k: warnings.warn('K is set to a value less than total voting groups!') distances = [] for group in data: for features in data[group]: euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict)) #欧几里得距离 distances.append([euclidean_distance,group]) votes = [i[1] for i in sorted(distances)[:k]] vote_result = Counter(votes).most_common(1)[0][0] #不使用[0][0],得到的是[('r', 3)]. [0][0]得到元组中第一个元素 return vote_resultresult = k_nearest_neighbors(dataset,new_features,k=3)print(result)plt.scatter(new_features[0],new_features[1],s=50,color=result)#预测的数据用小红点表示plt.show() 运行结果： 然后用这个代码来跑下癌症预测，代码如下 python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport matplotlib.pyplot as pltfrom matplotlib import styleimport warningsfrom collections import Counterimport pandas as pdimport randomdef k_nearest_neighbors(data, predict, k=3): if len(data) &gt;= k: warnings.warn('K is set to a value less than total voting groups!') distances = [] for group in data: for features in data[group]: euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict)) distances.append([euclidean_distance,group]) votes = [i[1] for i in sorted(distances)[:k]] vote_result = Counter(votes).most_common(1)[0][0] #不使用[0][0],得到的是[('r', 3)]. [0][0]得到元组中第一个元素 return vote_resultdf = pd.read_csv('breast-cancer-wisconsin.txt')df.replace('?',-99999,inplace=True)df.drop(['id'],1,inplace=True)full_data = df.astype(float).values.tolist()test_size = 0.2train_set = &#123;2:[], 4:[]&#125;#良性恶性两个labletest_set = &#123;2:[], 4:[]&#125;train_data = full_data[:-int(test_size*len(full_data))]test_data = full_data[-int(test_size*len(full_data)):] #最后20%correct = 0total = 0for i in train_data: train_set[i[-1]].append(i[:-1]) #去掉label，将属性填入for i in test_data: test_set[i[-1]].append(i[:-1])for group in test_set: for data in test_set[group]: vote = k_nearest_neighbors(train_set, data, k=5) if group == vote: correct += 1 total += 1print('Accuracy:', correct/total) 准确度很高！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>KNN</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode113]]></title>
    <url>%2F2020%2F02%2F08%2Fdfs%2F</url>
    <content type="text"><![CDATA[LCP 07. 首先用邻接存储每个人可以传送到的人。然后可以使用深度优先搜索，找出所有可能的传递方案。枚举每一轮传递玩家的编号和被传递玩家的编号。若当前是最后一轮且信息位于k 处，则方案总数加1。 c++123456789101112131415161718192021222324class Solution &#123;public: int res=0; map&lt;int,vector&lt;int&gt;&gt;g; int numWays(int n, vector&lt;vector&lt;int&gt;&gt;&amp; relation, int k) &#123; for(int i=0;i&lt;relation.size();i++)&#123; g[relation[i][0]].push_back(relation[i][1]); &#125; dfs(0,0,k,n); return res; &#125; void dfs(int i,int count,int k,int n)&#123; if(i==n-1&amp;&amp;count==k)&#123; res++; return; &#125; if(count&gt;k)return; if(g[i].size()==0)return; for(int j=0;j&lt;g[i].size();j++)&#123; dfs(g[i][j],count+1,k,n); &#125; &#125;&#125;; 括号生成 c++1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;string&gt; generateParenthesis(int n) &#123; vector&lt;string&gt;res; string cur="("; dfs(cur,n,res,1,0); return res; &#125; void dfs(string &amp;cur,int n,vector&lt;string&gt;&amp;res,int left,int right)&#123; if(cur.size()==2*n)&#123; res.push_back(cur); return; &#125; if(left&lt;n)&#123; cur=cur+'('; dfs(cur,n,res,left+1,right); cur.pop_back(); &#125; if(right&lt;left)&#123; cur=cur+')'; dfs(cur,n,res,left,right+1); cur.pop_back(); &#125; &#125;&#125;; 面试题13. 机器人的运动范围 c++123456789101112131415161718192021222324class Solution &#123;public: int res=0; int get(int n)&#123; int sums=0; while(n!=0)&#123; sums+=n%10; n/=10; &#125; return sums; &#125; int movingCount(int m, int n, int k) &#123; vector&lt;vector&lt;int&gt;&gt; cur(m, vector&lt;int&gt;(n, 0)); dfs(0,0,m,n,k,cur); return res; &#125; void dfs(int x,int y,int m, int n, int k,vector&lt;vector&lt;int&gt; &gt;&amp;cur) &#123; if(x&gt;=m||y&gt;=n||x&lt;0||y&lt;0||cur[x][y]==1||get(x)+get(y)&gt;k)return; res++; cur[x][y]=1; dfs(x+1,y,m,n,k,cur); dfs(x,y+1,m,n,k,cur); &#125;&#125;; python123456789101112131415161718class Solution &#123;public: int diameterOfBinaryTree(TreeNode* root) &#123; int res=0; dfs(root,res); return res; &#125; int dfs(TreeNode* root,int &amp;res)&#123; if(root==NULL)return 0; else &#123; int leftdepth=dfs(root-&gt;left,res); int rightdepth=dfs(root-&gt;right,res); res= max(res,leftdepth+rightdepth); return max(leftdepth,rightdepth)+1; &#125; &#125;&#125;; 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。 c++123456789class Solution &#123;public: bool hasPathSum(TreeNode* root, int sum) &#123; if(root==NULL)return false; sum=sum-root-&gt;val; if(root-&gt;left==NULL&amp;&amp;root-&gt;right==NULL&amp;&amp;sum==0)return true; return hasPathSum(root-&gt;left,sum)||hasPathSum(root-&gt;right,sum); &#125;&#125;; 对于113题，做一点思考，看下面两段代码： c++12345678910111213141516171819202122232425262728293031class Solution &#123; vector&lt;vector&lt;int&gt;&gt;res; vector&lt;int&gt;path;public: vector&lt;vector&lt;int&gt;&gt; pathSum(TreeNode* root, int sum) &#123; if(!root) return res; DFS(path,root,sum); return res; &#125; void DFS(vector&lt;int&gt;&amp;path,TreeNode* root, int sum) &#123; if(root==NULL) return; path.push_back(root-&gt;val); sum=sum-root-&gt;val; if(root-&gt;left==NULL&amp;&amp;root-&gt;right==NULL&amp;&amp;sum==0)res.push_back(path); else &#123; if(root-&gt;left) &#123; DFS(path,root-&gt;left,sum); path.pop_back(); &#125; if(root-&gt;right) &#123; DFS(path,root-&gt;right,sum); path.pop_back(); &#125; &#125; &#125;&#125;; c++123456789101112131415161718192021222324252627282930 class Solution &#123; vector&lt;vector&lt;int&gt;&gt;res; vector&lt;int&gt;path;public: vector&lt;vector&lt;int&gt;&gt; pathSum(TreeNode* root, int sum) &#123;//!!!!!! if(!root) return res; DFS(path,root,sum); return res; &#125; void DFS(vector&lt;int&gt;path,TreeNode* root, int sum) &#123; if(root==NULL) return; path.push_back(root-&gt;val); sum=sum-root-&gt;val; if(sum==0&amp;&amp;root-&gt;left==NULL&amp;&amp;root-&gt;right==NULL)res.push_back(path); else &#123; if(root-&gt;left) &#123; DFS(path,root-&gt;left,sum); //!!!!!! &#125; if(root-&gt;right) &#123; DFS(path,root-&gt;right,sum);//!!!!!! &#125; &#125; &#125;&#125;;![](2.png) 区别主要是，第一种每层递归函数都使用的一个容器，所以要加上引用，递归返回需要弹出之前的元素，而第二种是每次递归函数都复制一个容器。 17.电话号码的字母组合 c++12345678910111213141516171819202122232425262728class Solution &#123;public: map&lt;char,string&gt; mp=&#123;&#123;'2',"abc"&#125;,&#123;'3',"def"&#125;,&#123;'4',"ghi"&#125;,&#123;'5',"jkl"&#125;,&#123;'6',"mno"&#125;,&#123;'7',"pqrs"&#125;,&#123;'8',"tuv"&#125;,&#123;'9',"wxyz"&#125;&#125;; vector&lt;string&gt;res; void DFS(string cur,string next_word) &#123; if(next_word.size()==0) res.push_back(cur); else &#123; char digit=next_word[0]; string letters=mp[digit]; for(int i=0;i&lt;letters.size();i++) &#123; cur=cur+letters.substr(i,1); next_word=next_word.substr(1); DFS(cur,next_word); &#125; &#125; &#125; vector&lt;string&gt; letterCombinations(string digits)&#123; if(digits.size()==0) return &#123;&#125;; else DFS("",digits); return res; &#125;&#125;; 93.复原IP地址 python123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: vector&lt;string&gt; res; vector&lt;string&gt; restoreIpAddresses(string s) &#123; string temp; dfs(s,temp,0); return res; &#125; void dfs(string s,string &amp;temp,int num) &#123; if(num==4)&#123; if(s.empty())res.push_back(temp); &#125; else&#123; if(num&gt;0)temp+='.'; for(int i=1;i&lt;4&amp;&amp; i &lt;= s.length();i++)&#123; if(valid(s.substr(0,i))) &#123; temp=temp+s.substr(0,i); dfs(s.substr(i,s.length()-i),temp,num+1); temp.erase(temp.length()-i,i); &#125; &#125; temp.pop_back(); &#125; &#125; bool valid(const string&amp; s)&#123; if(s.empty() || (s[0] == '0' &amp;&amp; s.size()&gt;1) ) return false; int val = stoi(s); if(val &gt;= 0 &amp;&amp; val &lt;= 255) return true; return false; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>DFS</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pagerank]]></title>
    <url>%2F2020%2F02%2F07%2Fpagerank%2F</url>
    <content type="text"><![CDATA[The web as a graph pagerank是谷歌用来计算网页重要性的算法，我们把网页想象成节点，超链接想象成边，这就形成了一张有向图。 当然我们只考虑静态网页，不考虑防火墙拦截、无法访问这些情况。 两种有向图： 1.强连通图Strongly connected graphs：任意节点可以到达任意节点。 2.有向无环图Directed Acyclic Graph (DAG):首先没有环，u能达到v，但v不能达到u。 求strongly connected components (SCCs):对给定节点分别求入度和出度的BFS,然后对两个集合求交集 Bowtie structure of the web graph Broder et al. (1999) took a large snapshot of the web and tried to understand how the SCCs in the web graph fit together as a DAG 这张图Here the starting nodes are sorted by the number of nodes that BFS visits when starting from that node 图中蓝色节点只能访问一小部分节点，紫红色节点可以访问很多节点 通过这个我们可以得出网络中的图组成 PageRank - Ranking nodes on the graph 核心想法是吧links当作votes，一个节点的重要性是由被所指向的其他节点决定的 公式为rj=∑i→jridir_{j}= \sum _ {i \rightarrow j}\frac{r_{i}}{d_{i}}rj​=∑i→j​di​ri​​ Matrix formulation 这种计算方法需要N个式子，需要很长的时间。所以我们用邻接矩阵M来代替，M的每个列的和为1ifj→i,thenWij=1djif j \rightarrow i,then W_{^{ij}}=\frac{1}{d_{j}}ifj→i,thenWij​=dj​1​,则r=Mrr=Mrr=Mr，如下图的计算过程 之后进行不断地迭代，M(M...(M(Mr)M(M...(M(Mr)M(M...(M(Mr)直到∣r−r′∣&lt;ε|r-r^{&#x27;}|&lt; \varepsilon∣r−r′∣&lt;ε PageRank: Problems 1.dead ends:没有out-links 2.Spider traps：节点发出的边只有自环，最终会吸收所有的重要性，比如图中b会聚集所有的重要性，a会没有重要性 解决方法是random teleportation或者random jumps 当一次随机游走完成，下次网上冲浪有两种选择，有概率$\beta $跟随link，$1-\beta 跳到其它网页,跳到其他的网页节点有相同的可能性，跳到其它网页,跳到其他的网页节点有相同的可能性，跳到其它网页,跳到其他的网页节点有相同的可能性，\beta 通常设定在0.8到0.9综合起来就是:通常设定在0.8到0.9 综合起来就是:通常设定在0.8到0.9综合起来就是:r_{j}= \sum _ {i\rightarrow j}\frac{r_{i}}{d_{i}}+(1-\beta )\frac{1}{N}下面可以定义谷歌矩阵 下面可以定义谷歌矩阵下面可以定义谷歌矩阵A= \beta \times M+(1-\beta )[\frac{1}{N}]_ {N \times N}，，，r=A \times r$ 注意这个公式假设M没有dead ends。我们可以提前处理矩阵M去除dead ends或者使用概率为1的随机random teleports Computing PageRank: Sparse matrix formulation 但是这样对于节点太多的话，存储矩阵A(N×N)A(N\times N)A(N×N)需要大量的空间，我们可以这样来计算：r=βM×r+1−βNr=\beta M \times r+\frac{1-\beta }{N}r=βM×r+N1−β​ , 1−βN\frac{1-\beta }{N}N1−β​是一个向量，因为这样M是个稀疏矩阵，与向量乘机就没有那么大的计算量了 下面给出完整的算法流程]]></content>
      <categories>
        <category>图</category>
      </categories>
      <tags>
        <tag>pagerank</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gragh representation learning(图的表征学习)]]></title>
    <url>%2F2020%2F02%2F06%2Frepresentation-learning%2F</url>
    <content type="text"><![CDATA[核心思想：map each node in a network into a low-dimensional space把每个节点映射到低维空间 在学习一个网络表示的时候需要注意的几个性质： 适应性，网络表示必须能适应网络的变化。网络是一个动态的图，不断地会有新的节点和边添加进来，网络表示需要适应网络的正常演化。 属于同一个社区的节点有着类似的表示。网络中往往会出现一些特征相似的点构成的团状结构，这些节点表示成向量后必须相似。 低维。代表每个顶点的向量维数不能过高，过高会有过拟合的风险，对网络中有缺失数据的情况处理能力较差。 连续性。低维的向量应该是连续的。 Embedding Nodes node embedding的目标是在原网络的similarity近似于embedding space的相似度（内积） 1.定义一个encoder(i.e., a mapping from nodes to embeddings) 2.定义相似度函数（原始网络中的相似度） 3.优化encoder的参数，使得原始网络中u，v的相似度近似于embedding的点积 deep walk given a graph and a starting point, we select a neighbor of it at random, and move to this neighbor; then we select a neighbor of this point at random, and move to it, etc. The (random) sequence of points selected this way is a random walk on the graph. So similarity(u,v)similarity(u,v)similarity(u,v) is defined as the probability that u and v co-occur on a random walk over a network. 思想来源于语言模型，我们想要在所有训练短语中最大化概率Pr(wn∣w0,w1,....,wn−1)Pr(w_{n}|w_{0},w_{1},....,w_{n-1})Pr(wn​∣w0​,w1​,....,wn−1​)，The direct analog is to estimate the likelihood of observing vertex vi given all the previous vertices visited so far in the random walk. deep walk中算法主要包括两个部分，一个是random walk gengerator，第二个是更新程序. deep walk 论文笔记 random-walk embeddings有如下几步： 1.Estimate probability of visiting node v on a random walk starting from node u using some random walk strategy R. The simplest idea is just to run fixed-length, unbiased random walks starting from each node i.e., DeepWalk from Perozzi et al., 2013 2.Optimize embeddings to encode these random walk statistics, so the similarity between embeddings (e.g., dot product) encodes Random Walk similarity. deep walk配置 首先克隆代码到本地，进入目录CodeC:\Users\Administrator\Downloads\deepwalk-master```12. 执行```pip install -r requirements.txt 执行pythonsetup.py install```14. 执行```deepwalk --input example_graphs/karate.adjlist --output karate.embeddings embedding结果： 代码的解读：https://blog.csdn.net/github_36326955/article/details/82702379 Random walk optimization and Negative Sampling 待更新 Node2vec 考虑灵活变长的random walk，可以权衡网络的局部和全局的结构，有两种策略BFS,DFS 定义两个参数，p为返回之前节点的概率，q来调节DFS，BFS 算法： 1.估算random walk概率 2.对于每个节点u模拟r次长度为l的random walk 3.使用随机梯度下降进行更新]]></content>
      <categories>
        <category>图</category>
      </categories>
      <tags>
        <tag>embedding</tag>
        <tag>负采样，deep walk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hashtbale]]></title>
    <url>%2F2020%2F02%2F04%2Fhashtbale%2F</url>
    <content type="text"><![CDATA[题目49 给定一个字符串数组，将字母异位词组合在一起。字母异位词指字母相同，但排列不同的字符串。 思路：哈希表，对排序后的单词作为索引。 c++12345678910111213141516class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123; vector&lt;vector&lt;string&gt;&gt;res; map&lt;string,vector&lt;string&gt;&gt;cur; for(auto str:strs) &#123; string s=str; sort(s.begin(),s.end()); cur[s].push_back(str); &#125; for(auto it=cur.begin();it!=cur.end();it++) res.push_back(it-&gt;second); return res; &#125;&#125;; 题目169 给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。 c++123456789101112131415161718public: int majorityElement(vector&lt;int&gt;&amp; nums) &#123; map&lt;int,int&gt;res; int max=0,ans=0; for(int i=0;i&lt;nums.size();i++) &#123; res[nums[i]]++; if(res[nums[i]]&gt;max) &#123; max= res[nums[i]]; ans=nums[i]; &#125; &#125; return ans; &#125;&#125;; 方法二:因为出现最多的数出现超过了一半，所以随机选一个会很大概率是他 c++123456789101112131415class Solution &#123;public: int majorityElement(vector&lt;int&gt;&amp; nums) &#123; while (true) &#123; int candidate = nums[rand() % nums.size()]; int count = 0; for (int num : nums) if (num == candidate) ++count; if (count &gt; nums.size() / 2) return candidate; &#125; return -1; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>字符串</tag>
        <tag>哈希表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++编程总结(持续更新)]]></title>
    <url>%2F2020%2F02%2F03%2FC%2F</url>
    <content type="text"><![CDATA[字符串 删除末尾字符 1.str = str.substr(0, str.length() - 1); 2.str.erase(str.end() - 1); 3.str.pop_back(); 判断字符串里面是否含有某个字符串？ c++123456string a="abcdefghigklmn";string b="def";string::size_type idx;idx=a.find(b);//在a中查找b.if(idx == string::npos )//不存在。cout &lt;&lt; "not found\n"; 字符串替换 string.replace(起始index,结束index,代替字符串) c++1text.replace(i,j-i+1, dist[s]); leetcode1410 容器 vector获取值对应的索引 c++12vector&lt;int&gt;::iterator p=find(cur.begin(),cur.end(),queries[i]);auto index = std::distance(std::begin(cur), p); vector指定位置插入 c++1b.insert(b.begin(),q); 在首部插入 Code1234567891011# 图编程1.以u为顶点出发寻找与u，v能构成三角形的顶点w是，需要遍历u的临边，要注意过跳过v，添加if（v==w）continue2.变量的作用域重叠，导致后来定义会覆盖掉之前的，应定义新的变量3.入队出队寻找followers时，忘记对访问过的边做已访问标记，导致队列不会走空，进入无限循环，运行时间很久，诊断发现内存不断增加。# max_element() 和 min_element()在头文件 #include &lt;algorithm&gt; 中，返回的是迭代器，所以输出值的话要在前面加*例子：```c++vector&lt;int&gt;a;return *max_element(a.begin(),a.end()) VS使用 开启诊断 alt+ctrl+f2 2.找不到 #include &lt;graphics.h&gt;，下载easyx https://easyx.cn/]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.3]]></title>
    <url>%2F2020%2F02%2F03%2F2-3%2F</url>
    <content type="text"><![CDATA[题目402：给定一个以字符串表示的非负整数 num，移除这个数中的 k 位数字，使得剩下的数字最小。 测试用例 112, 思路要想使移除k个元素后的数最小，则应该移除最靠左的k个相邻逆序对，包括在一次移除后形成的新的逆序对. c++1234567891011121314151617181920class Solution &#123;public: string removeKdigits(string num, int k) &#123; string res; int n = num.size(), m = n - k; for (char c : num) &#123; while (k &amp;&amp; res.size() &amp;&amp; res.back() &gt; c) &#123; res.pop_back(); --k; &#125; res.push_back(c); &#125; res.resize(m); //去除前导0， 如10200，k = 1 while (!res.empty() &amp;&amp; res[0] == '0') &#123; res.erase(res.begin()); &#125; return res.empty() ? "0" : res; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>贪婪算法</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现regression]]></title>
    <url>%2F2020%2F02%2F01%2Fregression%2F</url>
    <content type="text"><![CDATA[两个error：bias,variance What to do with large bias? 1.Add more features as input 2.模型更复杂 What to do with large variance? 1.更多数据 2.增加正则化 梯度下降 学习率和损失函数的关系： 学习率大容易错过loss的最低点，学习率小下降慢 随着我们更新次数的增大，我们是希望我们的学习率越来越慢，因为分母是累加梯度的平方，到后面累加的比较大。因为我们认为在学习率的最初阶段，我们是距离损失函数最优解很远的，随着更新的次数的增多，我们认为越来越接近最优解，于是学习速率也随之变慢。 梯度下降理论 实际是用泰勒函数的近似 python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from statistics import meanimport numpy as npimport randomimport matplotlib.pyplot as pltfrom matplotlib import stylestyle.use('ggplot')def create_dataset(hm,variance,step=2,correlation=False): val = 1 ys = [] for i in range(hm): y = val + random.randrange(-variance,variance) ys.append(y) if correlation and correlation == 'pos': val+=step elif correlation and correlation == 'neg': val-=step xs = [i for i in range(len(ys))] return np.array(xs, dtype=np.float64),np.array(ys,dtype=np.float64)def best_fit_slope_and_intercept(xs,ys): m = (mean(xs)*mean(ys)-mean(xs*ys)) / (mean(xs)*mean(xs)-mean(xs*xs)) b= mean(ys)-m*mean(xs) return m,bdef squared_error(ys_orig,ys_line): return sum((ys_line - ys_orig) * (ys_line - ys_orig))def coefficient_of_determination(ys_orig,ys_line): y_mean_line = [mean(ys_orig) for y in ys_orig] squared_error_regr = sum((ys_line - ys_orig) * (ys_line - ys_orig)) squared_error_y_mean = sum((y_mean_line - ys_orig) * (y_mean_line - ys_orig)) print(squared_error_regr) print(squared_error_y_mean) r_squared = 1 - (squared_error_regr/squared_error_y_mean) return r_squaredxs, ys = create_dataset(40,40,2,correlation='pos')m, b = best_fit_slope_and_intercept(xs,ys)regression_line = [(m*x)+b for x in xs]r_squared = coefficient_of_determination(ys,regression_line)print(r_squared)plt.scatter(xs,ys,color='#003F72', label = 'data')plt.plot(xs, regression_line, label = 'regression line')plt.legend(loc=4)plt.show() R-square：分子是预测数据与原始数据均值之差的平方和，分母是原始数据和均值之差的平方和 R-square=0.5288792849075254]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国加油！]]></title>
    <url>%2F2020%2F01%2F28%2Fchina%2F</url>
    <content type="text"><![CDATA[时代的一粒灰，落在个人头上，就是一座山。]]></content>
  </entry>
  <entry>
    <title><![CDATA[博客报错总结]]></title>
    <url>%2F2020%2F01%2F21%2Ferror%2F</url>
    <content type="text"><![CDATA[github解决端口22不能连接错误 报错内容：ssh: connect to host github.com port 22: Connection timed out 解决方法： 打开这个文件C:\Program Files\Git\etc\ssh\ssh_config 添加以下内容：Host github.com User xxxxx@email.com Hostname ssh.github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa Port 443 报错内容： 出现了npm ERR! Error: EPERM: operation not permitted, open 'C:\Users\Administrator，npm-v显示bash: npm-v: command not found， 解决方法： 输入npm install hexo-deployer-git --save。]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gragh]]></title>
    <url>%2F2020%2F01%2F21%2Fgragh%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[linear regression实战（股票预测）]]></title>
    <url>%2F2020%2F01%2F14%2Fmachine-learning%2F</url>
    <content type="text"><![CDATA[从quandl获取股票数据(Open：开盘价Close：收盘价High：最高价Low：最低价Volume：成交量),留下有用的feature python12345678import pandas as pdimport quandldf =quandl.get("WIKI/GOOGL")df =df[['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close', 'Adj. Volume']]df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Close'] * 100.0df['PCT_change']=(df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']]print(df.head()) Adj. Close HL_PCT PCT_change Adj. Volume Date 2004-08-19 50.322842 8.072956 0.324968 44659000.0 2004-08-20 54.322689 7.921706 7.227007 22834300.0 2004-08-23 54.869377 4.049360 -1.227880 18256100.0 2004-08-24 52.597363 7.657099 -5.726357 15247300.0 fillna() 函数：有一个inplace参数，默认为false，不会对原来dataframe中进行替换，为True时候会修改原来的 python1234567forecast_col='Adj.Close'df.fillna(value=-9999,inplace=true)forecast_out = int(math.ceil(0.01 * len(df)))#比如现在有100天的数据，去预测未来一天的x=np.array(df.drop(['lable',1]) #当你要删除某一行或者某一列时，用drop函数，它不改变原有的df中的数据，而是返回另一个dataframe来存放删除后的数据y=np..array(df['labble'])X=preprocessing.scale(X) #特征在[-1,1] Fit(): Method calculates the parameters μ and σ and saves them as internal objects. 解释：简单来说，就是求得训练集X的均值啊，方差啊，最大值啊，最小值啊这些训练集X固有的属性。可以理解为一个训练过程 Transform(): Method using these calculated parameters apply the transformation to a particular dataset. 解释：在Fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如PCA，StandardScaler等）。 Fit_transform(): joins the fit() and transform() method for transformation of dataset. 解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。 python12345678910111213141516171819202122232425262728X_lately = X[-forecast_out:]X_lately=X[-forecast_out:]y = np.array(df['label'])print(len(X), len(y))X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)clf = svm.SVR()#kernel='poly'clf.fit(X_train, y_train)confidence = clf.score(X_test, y_test)forecast_set = clf.predict(X_lately)print(confidence,forecast_set)df['Forecast'] = np.nanlast_date = df.iloc[-1].name #iloc,loc:https://www.jianshu.com/p/f430d4f1b33flast_unix = last_date.timestamp() #转化为时间戳one_day = 86400next_unix = last_unix + one_dayfor i in forecast_set: next_date = datetime.datetime.fromtimestamp(next_unix) next_unix += 86400 df.loc[next_date]= [np.nan for _ in range(len(df.columns)-1)]+[i]df['Adj. Close'].plot()df['Forecast'].plot()plt.legend(loc=4)plt.xlabel('Date')plt.ylabel('Price')plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>regression</tag>
        <tag>应用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dp]]></title>
    <url>%2F2019%2F12%2F06%2Fdp%2F</url>
    <content type="text"><![CDATA[后无效性原则：当前啊状态只与上一个状态有关 0/1背包问题 一、 问题描述：有N件物品和一个容量为V的背包。第i件物品的费用（即体积，下同）是w[i]，价值是val[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。 二、 解题思路：用动态规划的思路，阶段就是“物品的件数”，状态就是“背包剩下的容量”，那么很显然f [ i , v ] 就设为从前 i 件物品中选择放入容量为 v 的背包最大的价值。那么有两种情况： 第i个物品大于背包容量，那么无法放入，dp[i][j]=dp[i-1][j] 第i个物品小于等于背包容量，可以放入，则比较两种情况即放入和不放入，如果不放入i，最大价值就和 i 无关，就是 dp[i-1][j] , 如果放入第 i 个物品，价值就是 dp[i-1][j-w[i]]+val[i]，我们只需取最大值即可,选择最优解dp[i][j]=max{ dp[i-1][j],dp[i-1][j-w[i]]+val[i] }。 三、优化 即对二维dp表优化 动态规划的一个原则： 后无效性原则：当前的状态只与上一个状态有关 面试题 17.16. 按摩师 最开始写的代码： c++123456789101112131415class Solution &#123;public: int massage(vector&lt;int&gt;&amp; nums) &#123; if(nums.size()==0)return 0; if(nums.size()==1) return nums[0]; if(nums.size()==2) return max(nums[0],nums[1]); vector&lt;int&gt;dp(nums.size(),0); dp[0]=nums[0]; dp[1]=max(nums[0],nums[1]); for(int i =2;i&lt;nums.size();i++)&#123; dp[i]=max(dp[i-2]+nums[i],dp[i-1]); &#125; return dp[nums.size()-1]; &#125;&#125;; 这种情况没有考虑到休息两天的，比如[2,1,1,2] 300. 最长上升组序列 转移方程 dp[i]=max(dp[i],dp[j]+1)if(nums[i]&gt;nums[j]) python1234567891011121314class Solution &#123;public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); if (n == 0) return 0; vector&lt;int&gt;dp(n,1); for(int i =1;i&lt;n;i++) for(int j=0;j&lt;i;j++)&#123; if(nums[i]&gt;nums[j]) dp[i]=max(dp[j]+1,dp[i]); &#125; return *max_element(dp.begin(), dp.end()); &#125;&#125;; 方法二： 维护一个 最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4], 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 c++123456789101112131415class Solution &#123;public: int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int dp=0,result=nums[0]; for(int i=0;i&lt;nums.size();i++)&#123; if(dp&gt;0) dp=dp+nums[i]; else dp=nums[i]; result=max(result,dp); &#125; return result; &#125;&#125;; 2.最小路径和 给定一个包含非负整数的 m x n 网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 说明：每次只能向下或者向右移动一步。 示例: 输入: [ [1,3,1], [1,5,1], [4,2,1] ] 输出: 7 解释: 因为路径 1→3→1→1→1 的总和最小。 c++1234567891011121314151617181920class Solution &#123;public: int minPathSum(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; if(grid.empty())return 0; int dp[grid.size()][grid[0].size()]; for(int i=0;i&lt;grid.size();i++)&#123; for(int j=0;j&lt;grid[0].size();j++) &#123; if(i==0&amp;&amp;j==0) dp[0][0]=grid[0][0]; else if(i==0) dp[i][j]= dp[i][j-1]+grid[i][j]; else if(j==0) dp[i][j]= dp[i-1][j]+grid[i][j]; else dp[i][j]=min(dp[i-1][j],dp[i][j-1])+grid[i][j]; &#125; &#125; return dp[grid.size()-1][grid[0].size()-1]; &#125;&#125;; 买卖股票的最佳时机 前i天的最大收益 = max{前i-1天的最大收益，第i天的价格-前i-1天中的最小价格} python123456789101112class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int res = 0; int min_val = 0x3f3f3f; //无穷大 for (int i = 0; i &lt; prices.size(); i++) &#123; min_val = min(min_val, prices[i]); res = max(res, prices[i] - min_val); &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node Classification]]></title>
    <url>%2F2019%2F11%2F05%2Fclassify%2F</url>
    <content type="text"><![CDATA[Node Classification 主要解决问题：给定一个网络，部分节点有标签，我们如何预测其它节点的标签。 例子：反欺诈案例，一些节点是欺诈者，一些节点是合法客户，我们怎么找到其它的欺诈者和合法客户。 补充： 图算法应用的任务有： node focused，以节点为主体，预测节点的标签等，比如上面的反欺诈的例子； edge focused，以边为主体，预测边的标签，比如推荐系统中 用户与商品是否发生关联（即用户是否购买商品），对这种关联关系是否发生进行预测； graph focused，以整个图为主题，预测图的标签等，比如化学分子的类别预测，整个化学分子是一副完整的图，其节点是不同的化学原子； 这节课主要讲节点分类的问题，也是目前图算法应用较多的领域，我们熟悉的gcn就是属于解决node focused任务的gnn的变体结构之一。 Collective Classification is an umbrella term describing how we assign labels to all nodes in the network together. We then propagate the information from these labels around the network and attempt to come up with stable assignments for each node. We are able to do these tasks because networks have special properties, specifically, correlations between nodes, that we can leverage to build our predictor. Essentially, collective classification relies on the Markov Assumption that the labely YiY_{i}Yi​ of one node depends on the labels of its neighbors, which can be mathematically written as:P(Yi∣i)=P(Yi∣Ni)P(Y_{i}|i)=P(Y_{i}|N_{i})P(Yi​∣i)=P(Yi​∣Ni​) The three main techniques that are used are Relational Classification, Iterative Classification, and Belief Classification, roughly ordered byhow advanced these methods are. Correlations in a Network Homophily Homophily generally refers to the tendency of individuals to associate and bond with similar others. Similarities, for instance in a social network, can include a variety of attributes, including age, gender, organizational affiliation, taste, and more. Influence Another example of why networks may demonstrate correlations is Influence. Under these circumstances, the links and edges formed can influence the behavior of the node itself Confounding Confounding variables can cause nodes to exhibit similar characteristics. For instance, the environment we are raised in may influence our similarity in multiple dimensions, from the language we speak, to our music tastes, to our political preferences. collective classification Whether or not a particular node X receives a particular label may depend on a variety of factors. In our context, those most commonly include: 未知样本的标签取决于： 这个未知样本O的特征； 这个未知样本O的相邻节点； 这个未知样本O的相邻节点的特征； 实际上如果仅仅考虑第一个条件就回到了我们传统的机器学习算法的范畴里了； 主要思想： 节点的标签是由其邻居的标签决定的 对于有标签的节点，就用ground-truth就好，没有标签的随机初始化，对于所有节点用随机的顺序更新直到收敛或者达到最大迭代次数 但是这种方法的问题在于： 1、无法保证一定能收敛（就是可能收敛也可能不收敛）（这里讲课的大佬给了一个经验法则，如果不收敛，但是随着时间的推移，不收敛的程度没有变大而是周期性的变动，那么这个时候我们也可以结束迭代） 2、模型仅仅使用的是节点的标签信息，但是没有用到节点的属性信息（特征）； Iterative Classification 第一种方案 relational classifiers 仅仅根据标签进行迭代，完全浪费了节点的属性信息，显然如果节点之间的属性非常相似，那么节点的标签也很可能是一样的，所以iterative classification 的思路就是同时利用节点的属性（特征矩阵）和标签； 其过程是： 为每一个节点创建一个向量形式（这里的意思应该是根据每个节点的属性得到一个特征向量） 使用分类器对得到的特征矩阵结合标签进行训练； 对于一个标签可能拥有许多的邻居，因此我们可以对其邻居的节点进行各类统计指标的计算加入特征中作为衍生特征，例如count计数、mode 求众数、proportion求占比、均值、是否存在的bool特征等； 这里详细介绍了iterative classifiers的整个过程： 首先是bootstrap phase，先使用特征矩阵来训练一个传统的机器学习模型比如svm、knn，然后预测标签，还是伪标签的思路； 然后是iteration phase，进入迭代步骤，对于每一个节点i都重复： 1. 更新特征向量ai； 2. 重新训练并且预测得到新的标签yi 一直到预测的概率整体不再变化或者变动不大或是达到了最大迭代次数； 同样，收敛也是无法保证的。 （这里补充了一点使用的知识，就是这类迭代的算法怎么去确定其停止条件，一个就是输出的值的收敛，理想状态是输出基本不发生改变，如果始终不收敛就看输出的差值的波动情况，如果是周期性在某个范围内波动而其差值不随迭代次数继续增大则可以选择输出差值较低的结果作为最终的收敛状态；另一个思路就比较简单了，设定最大迭代次数） Message Passing/Belief Propagation This equation summarizes our task: to calculate the message from i to j, we will sum over all of our states the label-label potential multiplied by our prior, multiplied by the product of all the messages sent by neighbors from the previous rounds. To initialize, we set all of our messages equal to 1. Then, we calculate our message from i to j, using the formula described above. We will repeat this for each node until we reach convergence, and then we can calculate our final assignment, i’s belief of being in state YiY_{i}Yi​ or b(Yi)b(Y_{i})b(Yi​)]]></content>
      <categories>
        <category>图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode1]]></title>
    <url>%2F2019%2F07%2F31%2Fleetcode1%2F</url>
    <content type="text"><![CDATA[2.两数相加 给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4) 输出：7 -&gt; 0 -&gt; 8 原因：342 + 465 = 807 c++12345678910111213141516171819202122232425class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; ListNode *answer = new ListNode(0); ListNode *curr= answer; int sum=0; int carry=0; while(l1!=NULL||l2!=NULL) &#123; int x=(l1!=NULL)?l1-&gt;val:0; int y=(l2!=NULL)?l2-&gt;val:0; sum=x+y+carry; curr-&gt;next= new ListNode(sum%10); curr=curr-&gt;next; carry=sum/10; if(l1!=NULL) l1=l1-&gt;next; if(l2!=NULL) l2=l2-&gt;next; &#125; if (carry &gt; 0) curr-&gt;next = new ListNode(carry); return answer-&gt;next; &#125;&#125;; 9.回文数 c++1234567class Solution(object): def isPalindrome(self, x): """ :type x: int :rtype: bool """ return str(x)==str(x)[::-1] 11盛水最多的容器 c++12345678910111213141516class Solution &#123;public: int maxArea(vector&lt;int&gt;&amp; height) &#123; int i=0,j=height.size()-1; int result=0; while(i&lt;j) &#123; result=max(result,(j-i)*min(height[i],height[j])); if(height[i]&gt;=height[j]) j--; else i++; &#125; return result; &#125;&#125;; 14.最长公共前缀 方法一： c++12345678910111213141516171819class Solution &#123;public: string longestCommonPrefix(vector&lt;string&gt;&amp; strs) &#123; if (strs.size()==0) return ""; string str; for (int i=0;i&lt;strs[0].size();i++)&#123; for(int k=0;k&lt;strs.size()-1;k++) &#123; if (strs[k][i]!=strs[k+1][i]) &#123; return str; &#125; &#125; str=str+strs[0][i]; &#125; return str; &#125;&#125;; 方法二： c++123class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: return os.path.commonprefix(strs) 方法三： c++123456789class Solution: def longestCommonPrefix(self, strs): if not strs: return "" s1 = min(strs) s2 = max(strs) for i,x in enumerate(s1): if x != s2[i]: return s2[:i] return s1 14.三数之和 c++123456789101112131415161718192021222324class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; int target; vector&lt;vector&lt;int&gt;&gt; ans; sort(nums.begin(), nums.end()); for (int i = 0; i &lt; nums.size(); i++) &#123; if ((target = nums[i]) &gt; 0) break; if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1]) continue; int l = i + 1, r = nums.size() - 1; while (l &lt; r) &#123; if (nums[l] + nums[r] + target &lt; 0) ++l; else if (nums[l] + nums[r] + target &gt; 0) --r; else &#123; ans.push_back(&#123;target, nums[l], nums[r]&#125;); ++l, --r; while (l &lt; r &amp;&amp; nums[l] == nums[l - 1]) ++l; while (l &lt; r &amp;&amp; nums[r] == nums[r + 1]) --r; &#125; &#125; &#125; return ans; &#125;&#125;; 16最近接的三数值和 c++123456789101112131415161718192021222324252627class Solution &#123; public: int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) &#123; sort(nums.begin(),nums.end()); int a=nums[0]+nums[1]+nums[2]; for(int i=0;i&lt;nums.size()-2;i++) &#123; if(i&gt;0&amp;&amp;nums[i]==nums[i-1])continue; int l=i+1; int r=nums.size()-1; while(l&lt;r) &#123; int sum=nums[i]+nums[l]+nums[r]; if(abs(sum-target)&lt;abs(a-target)) a=sum; if(sum&gt;target) r--; else if(sum&lt;target) l++; else if(sum==target) return sum; &#125; &#125; return a; &#125;&#125;; 19删除链表倒数第N个节点 快慢指针，相差n c++123456789101112131415161718192021class Solution &#123;public: ListNode* removeNthFromEnd(ListNode* head, int n) &#123; ListNode*p1=head; ListNode*p2=head; while(n!=0) &#123; p1=p1-&gt;next; n--; &#125; if(p1==NULL)return head-&gt;next; while((p1-&gt;next)!=NULL) &#123; p1=p1-&gt;next; p2=p2-&gt;next; &#125; p2-&gt;next=p2-&gt;next-&gt;next; return head; &#125;&#125;; 20.有效括号 c++1234567891011121314151617181920212223242526class Solution &#123;public: bool isValid(string s) &#123; stack&lt;char&gt;a; if(s.size()%2!=0)return 0; else&#123; for(auto i:s) &#123; if(i=='['||i=='&#123;'||i=='(') a.push(i); else if(a.size()==0&amp;&amp;(i==']'||i=='&#125;'||i==')')) return 0; else if((i==']'&amp;&amp;a.top()!='[')||(i=='&#125;'&amp;&amp;a.top()!='&#123;')||(i==')'&amp;&amp;a.top()!='(')) return 0; else a.pop(); &#125; &#125; if(a.size()!=0) return 0; else return 1; &#125;&#125;; 26删除排序数组中的重复项 双指针 c++1234567891011121314class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; if(nums.size()==0) return 0; int i=0; for(int j=0;j&lt;nums.size();j++) if(nums[i]!=nums[j]) &#123; nums[i+1]=nums[j]; i++; &#125; return i+1; &#125;&#125;; 28实现strSTR() c++1234567891011121314class Solution &#123;public: int strStr(string haystack, string needle) &#123; if (needle.size() == 0) return 0; if (needle.size() &gt; haystack.size()) return -1; if (needle==haystack)return 0; int len=needle.size(); for(int i=0;i&lt;haystack.size()-len+1;i++) &#123; if(needle==haystack.substr(i,len))return i; &#125; return -1; &#125;&#125;; 88.合并两个有序数组 从后向前插入数。 c++1234567891011121314151617class Solution &#123;public: void merge(vector&lt;int&gt;&amp; nums1, int m, vector&lt;int&gt;&amp; nums2, int n) &#123; int i=m-1; int j=n-1; int len=m+n-1; while(i&gt;=0&amp;&amp;j&gt;=0) &#123; if(nums1[i]&gt;nums2[j]) nums1[len--]=nums1[i--]; else nums1[len--]=nums2[j--]; &#125; while(j&gt;=0) nums1[len--]=nums2[j--]; &#125;&#125;; 颜色分类 方法一： c++12345678910111213141516171819202122class Solution &#123;public: void sortColors(vector&lt;int&gt;&amp; nums) &#123; int a=0,b=0,c=0; for(int i=0;i&lt;nums.size();i++) &#123; if(nums[i]==0) a++; if(nums[i]==1) b++; if(nums[i]==2) c++; &#125; int j=0; for(;j&lt;a;j++) nums[j]=0; for(;j&lt;b+a;j++) nums[j]=1; for(;j&lt;a+b+c;j++) nums[j]=2; &#125;&#125;; 方法二：荷兰国旗问题： https://en.wikipedia.org/wiki/Dutch_national_flag_problem c++1234567891011121314151617181920212223class Solution &#123;public: void sortColors(vector&lt;int&gt;&amp; nums) &#123; int p0=0,p2=nums.size()-1; int cur=0; while(cur&lt;=p2) &#123; if(nums[cur]==2) &#123; swap(nums[p2],nums[cur]); p2--; &#125; else if(nums[cur]==1) cur++; else if(nums[cur]==0) &#123; swap(nums[p0],nums[cur]); p0++,cur++; &#125; &#125; &#125;&#125;; 17.电话号码的字母组合 c++12345678910111213141516171819202122232425262728class Solution &#123;public: map&lt;char,string&gt; mp=&#123;&#123;'2',"abc"&#125;,&#123;'3',"def"&#125;,&#123;'4',"ghi"&#125;,&#123;'5',"jkl"&#125;,&#123;'6',"mno"&#125;,&#123;'7',"pqrs"&#125;,&#123;'8',"tuv"&#125;,&#123;'9',"wxyz"&#125;&#125;; vector&lt;string&gt;res; void DFS(string cur,string next_word) &#123; if(next_word.size()==0) res.push_back(cur); else &#123; char digit=next_word[0]; string letters=mp[digit]; for(int i=0;i&lt;letters.size();i++) &#123; cur=cur+letters.substr(i,1); next_word=next_word.substr(1); DFS(cur,next_word); &#125; &#125; &#125; vector&lt;string&gt; letterCombinations(string digits)&#123; if(digits.size()==0) return &#123;&#125;; else DFS("",digits); return res; &#125;&#125;;]]></content>
  </entry>
  <entry>
    <title><![CDATA[python数据可视化]]></title>
    <url>%2F2019%2F07%2F22%2Fpython-1%2F</url>
    <content type="text"><![CDATA[数据可视化是指通过可视化表示来搜索数据，与数据挖掘仅仅相关 一、安装matplotlib 进入命令行，输入pip install matplotlib便会自动安装。 二、绘制简单曲线 #coding=gbk import matplotlib.pyplot as plt import numpy as np squares=[1,4,9,16,25] plt.plot(squares,linewidth=5) #设置标题以及坐标标签 plt.title("Square Numbers",fontsize=24) plt.xlabel("Value",fontsize=14) plt.ylabel("Square of Value",fontsize=14) #设置刻度标记的大小 plt.tick_params(axis='both',labelsize=14) plt.show() ![](/images/图形1.jpg)]]></content>
  </entry>
  <entry>
    <title><![CDATA[heart]]></title>
    <url>%2F2019%2F05%2F03%2Fheart%2F</url>
    <content type="text"><![CDATA[该文章已加密, 请输入密码查看。 f9de2626922d11a4972db23cd0441146f601e8577a1934c355d1600e0fb79a07d19aa6c9b1b0ccbde383fa6097b432f33ebbb5116571d2b77d91e554d4ce27b339161eaac67137ce327f7b86c456e804c7134c9e2f4c068abdcd37ea663dd6b94ce11669707ad73e75985c7e82a2794b403a16a9fea6a483e5267ee5567dd2847f977a80438812a98d7e752e7f5078b23b3f124372fd2d92fe1801022d17e62b1433f6f5be7433da3ccf9f469f6170dbe9b8829ed5e198bb904cc55438cc7e5947d111347d1e78b7e328f15fcdb903fc2f25693a490d1d1131272f31dd113875]]></content>
      <categories>
        <category>心里话</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录一下]]></title>
    <url>%2F2019%2F04%2F28%2Fwanqing%2F</url>
    <content type="text"><![CDATA[哎]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k_means]]></title>
    <url>%2F2019%2F03%2F20%2Fk-means%2F</url>
    <content type="text"><![CDATA[简介 k-means algorithm算法是一个聚类算法，把n的对象根据他们的属性分为k个分割，k &lt; n。假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。通过迭代的方式将样本分到K个簇。 基本方法 选取K个点做为初始聚集的簇心（也可选择非样本点）; 分别计算每个样本点到 K个簇核心的距离（这里的距离一般取欧氏距离或余弦距离），找到离该点最近的簇核心，将它归属到对应的簇； 所有点都归属到簇之后， M个点就分为了 K个簇。之后重新计算每个簇的重心（平均距离中心），将其定为新的“簇核心”； 反复迭代 2 - 3 步骤，直到达到某个中止条件 sklearn实现 python1234567891011121314151617181920212223242526import matplotlib.pyplot as pltfrom sklearn.datasets import make_blobsfrom sklearn.cluster import KMeans#产生数据k=4X,Y = make_blobs(n_samples=100, n_features=2, centers=k)#构建模型km = KMeans(n_clusters=k, init='k-means++', max_iter=300)km.fit(X)# 获取簇心centroids = km.cluster_centers_# 获取归集后的样本所属簇对应值y_kmean = km.predict(X)print(y_kmean)# 呈现未归集前的数据plt.scatter(X[:, 0], X[:, 1], s=50)plt.yticks(())plt.show()plt.scatter(X[:, 0], X[:, 1], c=y_kmean, s=50, cmap='viridis')plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)plt.show() 手工实现 python12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npfrom sklearn.datasets import make_blobsfrom math import sqrtimport randomimport matplotlib.pyplot as pltfrom scipy import spatial#产生数据k=4X,Y = make_blobs(n_samples=100, n_features=2, centers=k)def calcuDistance(vec1, vec2): # 步骤1：定义欧式距离的公式 # 计算两个向量之间的欧式距离：根号下[(x_1-x_2)^2+(y_1-y_2)^2+...+(x_n-y_n)^2] # ver1 - ver2：表示两个向量的对应元素相减 return np.sqrt(np.sum(np.square(vec1 - vec2))) #注意这里的减号def k_means(data,k,Y): m, n = data.shape # m：样本数量，n：每个样本的属性值个数 cores = data[np.random.choice(np.arange(m), k, replace=False)] # 从m个数据样本中不重复地随机选择k个样本作为质心 print(cores) while True: # 迭代计算 #d = np.square(np.repeat(data, k, axis=0).reshape(m, k, n) - cores) #distance = np.sqrt(np.sum(d, axis=2)) # ndarray(m, k)，每个样本距离k个质心的距离，共有m行 distance = spatial.distance.cdist(data, cores,metric='euclidean') index_min = np.argmin(distance, axis=1) # 每个样本距离最近的质心索引序号 if (index_min == Y).all(): # 如果样本聚类没有改变 return Y, cores # 则返回聚类结果和质心数据 Y[:] = index_min # 重新分类 for i in range(k): # 遍历质心集 items = Y==i # 找出对应当前质心的子样本集 ，对应的items为[True,false......] cores[i] = np.mean(data[items], axis=0) # 以子样本集的均值作为当前质心的位置 result,cores=k_means(X,k,Y)plt.scatter(X[:, 0], X[:, 1], s=50)plt.yticks(())plt.show()print(Y)plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='viridis')plt.scatter(cores[:, 0], cores[:, 1], c='black', s=100, alpha=0.5)plt.show() k-means的改进 k-means改进的一个路线就是尽可能加快收敛速度，这个方向有几个思路： 1.质心初始化：选择初始质心之间有一些策略比如尽量远离，有助于反应数据的分布，加快收敛。 2.改进k-means的迭代过程，有几个方向，一个改进复杂度，比如数据的访问用KD树来索引，一个是改进目标函数（原始目标函数就是使同一类的离质心距离最小），有一个思路是时刻更新质心，比如移动一个样本到最近的类别，就立刻更新相应的两个类质心，这样改变了每轮都要对所有样本更新label的繁琐过程。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2013年浙江大学复试机试模拟题]]></title>
    <url>%2F2019%2F03%2F15%2Foj%2F</url>
    <content type="text"><![CDATA[题目描述 Xiao Ming’s parents visit ZJU and Xiao Ming want to take them to look around the campus.They will start from the stone with two famous question raised by President Zhu Kezhen and end at largest dining room in Asia.They want to visit every place exactly once in ZJU’s campus,including the stone and dining room. 输入 The input consists of multiple test cases. The first line contains an integer n(n&lt;=20),which means the number of place in ZJU’s campus.We give numbers(from 1 to n ) to the places,especailly,1 means the stone with two famous question and n means the largest dining room. The second line contains an integer m,which means the number of roads between two place. Then follows m lines,each line contain two integer,which means there is a road between these two place.The road will not repeat more than one time. 输出 For each test case, you should output one line.If the path exists,you should output 1.Otherwise,you should output 0. 样例输入 5 4 1 2 1 3 1 4 2 5 6 6 1 3 3 2 1 2 3 4 4 5 5 6 样例输出 0 1 来源 2013年浙江大学复试机试模拟题 #include&lt;stdio.h&gt; #include&lt;string.h&gt; int n,m,ok; int vis[22],Map[22][22]; //搜索，已经访问count个地方现在处于location点 void DFS(int location,int count) { int i; //已经全部访问完 if(count == n){ //到达目的地n if(location == n){ ok = 1; } return; } //没有访问完，访问下一处 for(i = 1;i &lt;= n;i++){ //i点没访问过且能访问则去i点 if(Map[location][i] == 1&amp;&amp; vis[i] == 0){ //标记i已经访问过 vis[i]=1; //递归下一处 DFS(i,count+1); if(ok == 1){ return; } //取消标记 vis[i] = 0; } } } //初始化 void Init() { int i,j,start,end; //初始化地图 for(i = 1;i &lt;= n;i++){ for(j = 1;j &lt;= n;j++){ Map[i][j]=0; } } //添加路况 for(i = 0;i &lt; m;i++){ scanf(&quot;%d %d&quot;,&amp;start,&amp;end); //end和start之间联通 Map[start][end]=1; Map[end][start]=1; } memset(vis,0,sizeof(vis)); ok = 0; //1为出发点 vis[1]=1; } int main() { while(scanf(&quot;%d %d&quot;,&amp;n,&amp;m)!=EOF){ Init(); DFS(1,1); printf(&quot;%d\n&quot;,ok); } return 0; }]]></content>
      <categories>
        <category>算法编程</category>
      </categories>
  </entry>
</search>

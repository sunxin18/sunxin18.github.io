<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>word2vec | lalala</title><meta name="description" content="word2vec"><meta name="keywords" content="skipgram"><meta name="author" content="Sunxin"><meta name="copyright" content="Sunxin"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="word2vec"><meta name="twitter:description" content="word2vec"><meta name="twitter:image" content="https://sunxin18.github.io/img/14.png"><meta property="og:type" content="article"><meta property="og:title" content="word2vec"><meta property="og:url" content="https://sunxin18.github.io/2020/03/06/skipgram/"><meta property="og:site_name" content="lalala"><meta property="og:description" content="word2vec"><meta property="og:image" content="https://sunxin18.github.io/img/14.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://sunxin18.github.io/2020/03/06/skipgram/"><link rel="prev" title="gragh_convolution" href="https://sunxin18.github.io/2020/03/08/gragh-convolution/"><link rel="next" title="leetcode_day" href="https://sunxin18.github.io/2020/03/02/leetcode-day/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: {"text":"富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善","fontSize":"15px"},
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">lalala</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-git" aria-hidden="true"></i><span> 娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/playlist/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/books"><i class="fa-fw fa fa-book"></i><span> 读书</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 旅行</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/html/love1"><i class="fa-fw fa fa-heartbeat"></i><span> 爱</span></a></li><li><a class="site-page" href="/html/在线走迷宫"><i class="fa-fw fa fa-angellist"></i><span> 在线走迷宫</span></a></li></ul></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">80</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">61</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-git" aria-hidden="true"></i><span> 娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/playlist/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/books"><i class="fa-fw fa fa-book"></i><span> 读书</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 旅行</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/html/love1"><i class="fa-fw fa fa-heartbeat"></i><span> 爱</span></a></li><li><a class="site-page" href="/html/在线走迷宫"><i class="fa-fw fa fa-angellist"></i><span> 在线走迷宫</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#自然语言模型的发展与引出"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> 自然语言模型的发展与引出</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#模型解析"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> 模型解析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#概念"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text"> 概念</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#权重矩阵"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text"> 权重矩阵</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#输出层"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text"> 输出层</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#代码"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text"> 代码</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#评估词向量"><span class="toc_mobile_items-number">2.5.</span> <span class="toc_mobile_items-text"> 评估词向量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#词向量局限性"><span class="toc_mobile_items-number">2.6.</span> <span class="toc_mobile_items-text"> 词向量局限性</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#自然语言模型的发展与引出"><span class="toc-number">1.</span> <span class="toc-text"> 自然语言模型的发展与引出</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#模型解析"><span class="toc-number">2.</span> <span class="toc-text"> 模型解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#概念"><span class="toc-number">2.1.</span> <span class="toc-text"> 概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#权重矩阵"><span class="toc-number">2.2.</span> <span class="toc-text"> 权重矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#输出层"><span class="toc-number">2.3.</span> <span class="toc-text"> 输出层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-number">2.4.</span> <span class="toc-text"> 代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#评估词向量"><span class="toc-number">2.5.</span> <span class="toc-text"> 评估词向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#词向量局限性"><span class="toc-number">2.6.</span> <span class="toc-text"> 词向量局限性</span></a></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/img/14.png)"><div id="post-info"><div id="post-title"><div class="posttitle">word2vec</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-03-06<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-07-28</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/自然语言处理/">自然语言处理</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="自然语言模型的发展与引出"><a class="markdownIt-Anchor" href="#自然语言模型的发展与引出"></a> 自然语言模型的发展与引出</h1>
<p><a href="https://www.cnblogs.com/guoyaohua/p/9240336.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9240336.html</a><br>
基于频率或者预测模型：<a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a><br>
语言模型会给一个正常的语句很高的概率<br>
unigram:<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_{1},w_{2}...w_{n})=\prod_{i=1}^{n}P(w_{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.104002em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><br>
然而下一个词很大程度会取决于前面序列的词，这样独立概率会让一些愚蠢的语句也可以得到很大的值，所以引出了<br>
Bigram model:<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></msubsup><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_{1},w_{2}...w_{n})=\prod_{i=2}^{n}P(w_{i}|w_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.104002em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<h1 id="模型解析"><a class="markdownIt-Anchor" href="#模型解析"></a> 模型解析</h1>
<h2 id="概念"><a class="markdownIt-Anchor" href="#概念"></a> 概念</h2>
<p>语料(corpus)是指文本所有内容，包括重复的词，词典<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span>是从语料中抽取出来的不包括重复词语<br>
one-hot这种表示方式使得每一个词映射到高维空间中都是互相正交的，也就是说one-hot向量空间中词与词之间没有任何关联关系，这显然与实际情况不符合，因为实际中词与词之间有近义、反义等多种关系。Word2vec虽然学习不到反义这种高层次语义信息，但它巧妙的运用了一种思想：“具有相同上下文的词语包含相似的语义”，使得语义相近的词在映射到欧式空间后中具有较高的余弦相似度</p>
<p>其实总体思想还是降维，one-hot表达维度太大了，svd矩阵分解两个低纬度矩阵。</p>
<h2 id="权重矩阵"><a class="markdownIt-Anchor" href="#权重矩阵"></a> 权重矩阵</h2>
<p><a href="https://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="noopener">https://blog.csdn.net/itplus/article/details/37969979</a><br>
以skipgram为例主要有两个权重矩阵,第一个是中心词的向量表达矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>,第二个是上下文单词的向量表达矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span></span></span></span>,他们都是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi><mo>×</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">D\times V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>维的<br>
下面总结一下计算过程:<br>
1.首先输入中心词<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\omega_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的one-hot编码(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">V\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>)<br>
2.接着与矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>运算得到中心词的representation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mi>c</mi></msub><mo>=</mo><msub><mi>ω</mi><mi>t</mi></msub><mo>⋅</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v_{c}=\omega_{t}\cdot V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.59445em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span> (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">D\times1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>)<br>
3.下一步就是中心词向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">v_{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span></span></span></span>相乘（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>u</mi><mn>0</mn><mi>T</mi></msubsup><msub><mi>v</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">u_{0}^{T}v_{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894389999999998em;vertical-align:-0.24810799999999997em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>u</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">u_{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>就是矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span></span></span></span>的某一行，其实这个就是即某个上下文词的one-hot的表达乘以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span></span></span></span>得到representation,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>u</mi><mn>0</mn><mi>T</mi></msubsup><msub><mi>v</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">u_{0}^{T}v_{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894389999999998em;vertical-align:-0.24810799999999997em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>这最后得到的就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">V\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>向量的一维)<br>
可以听cs24n的视频讲解<br>
4. 下面用softmax把相似性大小转变为概率<br>
<a href="/2020/03/06/skipgram/1.png" data-fancybox="group" data-caption class="fancybox"><img src="/2020/03/06/skipgram/1.png" alt title></a><br>
详细的一个例子：<a href="https://cloud.tencent.com/developer/article/1591734" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1591734</a></p>
<h2 id="输出层"><a class="markdownIt-Anchor" href="#输出层"></a> 输出层</h2>
<p>对应一颗二叉树，词典中的词作为叶子节点，根据单词出现次数作为权值，构造huffman树，叶子节点一共<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>D</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|D|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord">∣</span></span></span></span>个,每一次分支都是二分类，分到左面是负类，右面是正类，<a href="https://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="noopener">详细过程</a>,<a href="https://www.cnblogs.com/neopenx/p/4571996.html" target="_blank" rel="noopener">https://www.cnblogs.com/neopenx/p/4571996.html</a></p>
<h2 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h2>
<p><a href="https://github.com/chrisjmccormick/word2vec_commented" target="_blank" rel="noopener">源代码</a><br>
#构造一个神经网络，输入词语，输出词向量</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练数据</span></span><br><span class="line"><span class="comment">#text = "I like dog i like cat i like animal dog cat animal apple cat dog like dog fish milk like dog \</span></span><br><span class="line"><span class="comment">#cat eyes like i like apple apple i hate apple i movie book music like cat dog hate cat dog like"</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/content/text8'</span>) <span class="keyword">as</span> f: <span class="comment">#colab上的路径</span></span><br><span class="line">    text = f.read()</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数设置</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">2</span> <span class="comment">#词向量维度</span></span><br><span class="line">PRINT_EVERY = <span class="number">1000</span> <span class="comment">#可视化频率</span></span><br><span class="line">EPOCHS = <span class="number">3</span> <span class="comment">#训练的轮数</span></span><br><span class="line">BATCH_SIZE = <span class="number">5</span> <span class="comment">#每一批训练数据大小</span></span><br><span class="line">N_SAMPLES = <span class="number">3</span> <span class="comment">#负样本大小</span></span><br><span class="line">WINDOW_SIZE = <span class="number">5</span> <span class="comment">#周边词窗口大小</span></span><br><span class="line">FREQ = <span class="number">0</span> <span class="comment">#词汇出现频率</span></span><br><span class="line">DELETE_WORDS = <span class="keyword">False</span> <span class="comment">#是否删除部分高频词</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#文本预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text, FREQ)</span>:</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    words = text.split()</span><br><span class="line">    <span class="comment">#去除低频词</span></span><br><span class="line">    word_counts = Counter(words)</span><br><span class="line">    trimmed_words = [word <span class="keyword">for</span> word <span class="keyword">in</span> words <span class="keyword">if</span> word_counts[word] &gt; FREQ]</span><br><span class="line">    <span class="keyword">return</span> trimmed_words</span><br><span class="line">words = preprocess(text, FREQ)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建词典</span></span><br><span class="line">vocab = set(words)</span><br><span class="line">vocab2int = &#123;w: c <span class="keyword">for</span> c, w <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">int2vocab = &#123;c: w <span class="keyword">for</span> c, w <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#将文本转化为数值</span></span><br><span class="line">int_words = [vocab2int[w] <span class="keyword">for</span> w <span class="keyword">in</span> words]</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算单词频次</span></span><br><span class="line">int_word_counts = Counter(int_words)</span><br><span class="line">total_count = len(int_words)</span><br><span class="line">word_freqs = &#123;w: c/total_count <span class="keyword">for</span> w, c <span class="keyword">in</span> int_word_counts.items()&#125;<span class="comment">#items()方法把字典中每对key和value组成一个元组，并把这些元组放在列表中返回。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#去除出现频次高的词汇</span></span><br><span class="line"><span class="keyword">if</span> DELETE_WORDS:</span><br><span class="line">    t = <span class="number">1e-5</span></span><br><span class="line">    prob_drop = &#123;w: <span class="number">1</span>-np.sqrt(t/word_freqs[w]) <span class="keyword">for</span> w <span class="keyword">in</span> int_word_counts&#125;</span><br><span class="line">    train_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> int_words <span class="keyword">if</span> random.random()&lt;(<span class="number">1</span>-prob_drop[w])]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    train_words = int_words</span><br><span class="line"></span><br><span class="line"><span class="comment">#单词分布</span></span><br><span class="line">word_freqs = np.array(list(word_freqs.values()))</span><br><span class="line">unigram_dist = word_freqs / word_freqs.sum()</span><br><span class="line">noise_dist = torch.from_numpy(unigram_dist ** (<span class="number">0.75</span>) / np.sum(unigram_dist ** (<span class="number">0.75</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取目标词汇</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_target</span><span class="params">(words, idx, WINDOW_SIZE)</span>:</span></span><br><span class="line">  target_window = np.random.randint(<span class="number">1</span>, WINDOW_SIZE+<span class="number">1</span>)</span><br><span class="line">  start_point = idx-target_window <span class="keyword">if</span> (idx-target_window)&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">  end_point = idx+target_window</span><br><span class="line">  targets = set(words[start_point:idx]+words[idx+<span class="number">1</span>:end_point+<span class="number">1</span>])</span><br><span class="line">  <span class="keyword">return</span> list(targets)</span><br><span class="line"></span><br><span class="line"><span class="comment">#批次化数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(words, BATCH_SIZE, WINDOW_SIZE)</span>:</span></span><br><span class="line">  n_batches = len(words)//BATCH_SIZE</span><br><span class="line">  words = words[:n_batches*BATCH_SIZE]</span><br><span class="line">  <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">0</span>, len(words), BATCH_SIZE):</span><br><span class="line">    batch_x, batch_y = [],[]</span><br><span class="line">    batch = words[idx:idx+BATCH_SIZE]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(batch)):</span><br><span class="line">      x = batch[i]</span><br><span class="line">      y = get_target(batch, i, WINDOW_SIZE)</span><br><span class="line">      batch_x.extend([x]*len(y))</span><br><span class="line">      batch_y.extend(y)</span><br><span class="line">    <span class="keyword">yield</span> batch_x, batch_y</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGramNeg</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_vocab, n_embed, noise_dist)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.n_vocab = n_vocab</span><br><span class="line">        self.n_embed = n_embed</span><br><span class="line">        self.noise_dist = noise_dist</span><br><span class="line">        <span class="comment">#定义词向量层</span></span><br><span class="line">        self.in_embed = nn.Embedding(n_vocab, n_embed)</span><br><span class="line">        self.out_embed = nn.Embedding(n_vocab, n_embed)</span><br><span class="line">        <span class="comment">#词向量层参数初始化</span></span><br><span class="line">        self.in_embed.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.out_embed.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#输入词的前向过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_input</span><span class="params">(self, input_words)</span>:</span></span><br><span class="line">        input_vectors = self.in_embed(input_words)</span><br><span class="line">        <span class="keyword">return</span> input_vectors</span><br><span class="line">    <span class="comment">#目标词的前向过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_output</span><span class="params">(self, output_words)</span>:</span></span><br><span class="line">        output_vectors = self.out_embed(output_words)</span><br><span class="line">        <span class="keyword">return</span> output_vectors</span><br><span class="line">    <span class="comment">#负样本词的前向过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_noise</span><span class="params">(self, size, N_SAMPLES)</span>:</span></span><br><span class="line">        noise_dist = self.noise_dist</span><br><span class="line">        <span class="comment">#从词汇分布中采样负样本</span></span><br><span class="line">        noise_words = torch.multinomial(noise_dist,</span><br><span class="line">                                        size * N_SAMPLES,</span><br><span class="line">                                        replacement=<span class="keyword">True</span>)</span><br><span class="line">        noise_vectors = self.out_embed(noise_words).view(size, N_SAMPLES, self.n_embed)</span><br><span class="line">        <span class="keyword">return</span> noise_vectors</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NegativeSamplingLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_vectors, output_vectors, noise_vectors)</span>:</span></span><br><span class="line">        BATCH_SIZE, embed_size = input_vectors.shape</span><br><span class="line">        <span class="comment">#将输入词向量与目标词向量作维度转化处理</span></span><br><span class="line">        input_vectors = input_vectors.view(BATCH_SIZE, embed_size, <span class="number">1</span>)</span><br><span class="line">        output_vectors = output_vectors.view(BATCH_SIZE, <span class="number">1</span>, embed_size)</span><br><span class="line">        <span class="comment">#目标词损失</span></span><br><span class="line">        test = torch.bmm(output_vectors, input_vectors)</span><br><span class="line">        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()</span><br><span class="line">        out_loss = out_loss.squeeze()</span><br><span class="line">        <span class="comment">#负样本损失</span></span><br><span class="line">        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()</span><br><span class="line">        noise_loss = noise_loss.squeeze().sum(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#综合计算两类损失</span></span><br><span class="line">        <span class="keyword">return</span> -(out_loss + noise_loss).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型、损失函数及优化器初始化</span></span><br><span class="line">model = SkipGramNeg(len(vocab2int), EMBEDDING_DIM, noise_dist=noise_dist)</span><br><span class="line">criterion = NegativeSamplingLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.003</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">steps = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">    <span class="comment">#获取输入词以及目标词</span></span><br><span class="line">    <span class="keyword">for</span> input_words, target_words <span class="keyword">in</span> get_batch(train_words, BATCH_SIZE, WINDOW_SIZE):</span><br><span class="line">        steps += <span class="number">1</span></span><br><span class="line">        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)</span><br><span class="line">        <span class="comment">#输入、输出以及负样本向量</span></span><br><span class="line">        input_vectors = model.forward_input(inputs)</span><br><span class="line">        output_vectors = model.forward_output(targets)</span><br><span class="line">        size, _ = input_vectors.shape</span><br><span class="line">        noise_vectors = model.forward_noise(size, N_SAMPLES)</span><br><span class="line">        <span class="comment">#计算损失</span></span><br><span class="line">        loss = criterion(input_vectors, output_vectors, noise_vectors)</span><br><span class="line">        <span class="comment">#打印损失</span></span><br><span class="line">        <span class="keyword">if</span> steps%PRINT_EVERY == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"loss："</span>,loss)</span><br><span class="line">        <span class="comment">#梯度回传</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化词向量</span></span><br><span class="line"><span class="keyword">for</span> i, w <span class="keyword">in</span> int2vocab.items() :</span><br><span class="line">    vectors = model.state_dict()[<span class="string">"in_embed.weight"</span>]</span><br><span class="line">    x,y = float(vectors[i][<span class="number">0</span>]),float(vectors[i][<span class="number">1</span>])</span><br><span class="line">    plt.scatter(x,y)</span><br><span class="line">    plt.annotate(w, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">'offset points'</span>, ha=<span class="string">'right'</span>, va=<span class="string">'bottom'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div>
<h2 id="评估词向量"><a class="markdownIt-Anchor" href="#评估词向量"></a> 评估词向量</h2>
<ol>
<li>可视化</li>
<li>相似度计算（余弦）</li>
<li>analogy类比</li>
</ol>
<h2 id="词向量局限性"><a class="markdownIt-Anchor" href="#词向量局限性"></a> 词向量局限性</h2>
<ol>
<li>一词多义</li>
<li></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Sunxin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sunxin18.github.io/2020/03/06/skipgram/">https://sunxin18.github.io/2020/03/06/skipgram/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sunxin18.github.io">lalala</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/skipgram/">skipgram    </a></div><div class="post_share"><div class="social-share" data-image="/img/14.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/uploads/wechat-qcode.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/uploads/zhifubao.jpg" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/03/08/gragh-convolution/"><img class="prev_cover lazyload" data-src="/img/15.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>gragh_convolution</span></div></a></div><div class="next-post pull_right"><a href="/2020/03/02/leetcode-day/"><img class="next_cover lazyload" data-src="https://i.loli.net/2020/05/21/mc8v4pVDytnoFAT.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>leetcode_day</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="lv-container" data-id="city" data-uid="MTAyMC80NDA0NC8yMDU3OQ"><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By Sunxin</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎您的访问！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/click_heart.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/ClickShowText.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>